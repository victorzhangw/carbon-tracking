#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€²éšéŸ³é »äººè²åˆ†é›¢å’Œèªªè©±è€…åˆ†é›¢å·¥å…·
å°ˆé–€é‡å°å°è©±å ´æ™¯ï¼Œä¿æŒäººè²æ¸…æ™°åº¦ä¸¦åˆ†é›¢ä¸åŒèªªè©±è€…
"""

import os
import sys
import librosa
import soundfile as sf
import numpy as np
from scipy import signal
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.spatial.distance import pdist
import noisereduce as nr
from pydub import AudioSegment
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import argparse
from pathlib import Path
import matplotlib.pyplot as plt

class AdvancedVoiceSeparator:
    def __init__(self, input_file):
        self.input_file = input_file
        self.output_dir = Path(input_file).parent / "advanced_separated_audio"
        self.output_dir.mkdir(exist_ok=True)
        
    def convert_to_wav(self):
        """å°‡ m4a æˆ–å…¶ä»–æ ¼å¼è½‰æ›ç‚º wavï¼Œä¿æŒé«˜å“è³ª"""
        try:
            # ä½¿ç”¨æ›´é«˜çš„å“è³ªè¨­å®š
            audio = AudioSegment.from_file(self.input_file)
            # ç¢ºä¿æ¡æ¨£ç‡è¶³å¤ é«˜ä»¥ä¿æŒèªéŸ³å“è³ª
            if audio.frame_rate < 22050:
                audio = audio.set_frame_rate(22050)
            wav_path = self.output_dir / f"{Path(self.input_file).stem}_converted.wav"
            audio.export(wav_path, format="wav", parameters=["-q:a", "0"])
            print(f"âœ“ éŸ³é »å·²è½‰æ›ç‚ºé«˜å“è³ª WAV æ ¼å¼: {wav_path}")
            return str(wav_path)
        except Exception as e:
            print(f"âœ— è½‰æ›å¤±æ•—: {e}")
            return None
    
    def load_audio(self, file_path):
        """è¼‰å…¥éŸ³é »æª”æ¡ˆï¼Œä¿æŒåŸå§‹æ¡æ¨£ç‡"""
        try:
            # ä¿æŒåŸå§‹æ¡æ¨£ç‡ä»¥ç¶­æŒå“è³ª
            y, sr = librosa.load(file_path, sr=None)
            print(f"âœ“ éŸ³é »è¼‰å…¥æˆåŠŸ - æ¡æ¨£ç‡: {sr}Hz, é•·åº¦: {len(y)/sr:.2f}ç§’")
            return y, sr
        except Exception as e:
            print(f"âœ— éŸ³é »è¼‰å…¥å¤±æ•—: {e}")
            return None, None
    
    def advanced_noise_reduction(self, y, sr):
        """é€²éšå™ªéŸ³æ¸›å°‘ï¼Œå°ˆé–€è™•ç†éˆ´è²å’Œç©ºç™½ç‰‡æ®µ"""
        try:
            print("é–‹å§‹é€²éšå™ªéŸ³è™•ç†...")
            
            # 1. æª¢æ¸¬å’Œå»é™¤éˆ´è²ï¼ˆé€šå¸¸åœ¨ç‰¹å®šé »ç‡ç¯„åœï¼‰
            y_no_ring = self.remove_ring_tones(y, sr)
            
            # 2. å»é™¤ç©ºç™½å’Œä½èƒ½é‡ç‰‡æ®µ
            y_trimmed = self.remove_silence_segments(y_no_ring, sr)
            
            # 3. å¤šéšæ®µå™ªéŸ³æ¸›å°‘
            # ç¬¬ä¸€éšæ®µï¼šå¼·åŠ›å»é™¤ç©©å®šå™ªéŸ³
            y_stage1 = nr.reduce_noise(
                y=y_trimmed, 
                sr=sr, 
                stationary=True, 
                prop_decrease=0.7,  # è¼ƒå¼·çš„æ¸›å°‘
                n_std_thresh_stationary=1.0,
                n_fft=2048
            )
            
            # ç¬¬äºŒéšæ®µï¼šå»é™¤éç©©å®šå™ªéŸ³
            y_stage2 = nr.reduce_noise(
                y=y_stage1, 
                sr=sr, 
                stationary=False, 
                prop_decrease=0.5,
                n_std_thresh_stationary=1.2,
                n_fft=1024
            )
            
            # 4. é »åŸŸæ¿¾æ³¢å»é™¤ç‰¹å®šå™ªéŸ³
            y_filtered = self.apply_noise_filters(y_stage2, sr)
            
            print("âœ“ é€²éšå™ªéŸ³æ¸›å°‘å®Œæˆ")
            return y_filtered
            
        except Exception as e:
            print(f"âœ— å™ªéŸ³æ¸›å°‘å¤±æ•—: {e}")
            return y
    
    def remove_ring_tones(self, y, sr):
        """æª¢æ¸¬å’Œå»é™¤éˆ´è²"""
        try:
            # éˆ´è²é€šå¸¸åœ¨ç‰¹å®šé »ç‡ç¯„åœï¼Œä½¿ç”¨å¸¶é˜»æ¿¾æ³¢å™¨
            nyquist = sr / 2
            
            # å¸¸è¦‹éˆ´è²é »ç‡ç¯„åœ
            ring_frequencies = [
                (400, 500),   # ä½é »éˆ´è²
                (800, 1000),  # ä¸­é »éˆ´è²  
                (1400, 1600), # é«˜é »éˆ´è²
                (2000, 2200)  # è¶…é«˜é »éˆ´è²
            ]
            
            y_filtered = y.copy()
            
            for low_freq, high_freq in ring_frequencies:
                if high_freq < nyquist:
                    # å‰µå»ºå¸¶é˜»æ¿¾æ³¢å™¨
                    sos = signal.butter(4, [low_freq, high_freq], 
                                      btype='bandstop', fs=sr, output='sos')
                    y_filtered = signal.sosfilt(sos, y_filtered)
            
            print("âœ“ éˆ´è²å»é™¤å®Œæˆ")
            return y_filtered
            
        except Exception as e:
            print(f"âœ— éˆ´è²å»é™¤å¤±æ•—: {e}")
            return y
    
    def remove_silence_segments(self, y, sr):
        """å»é™¤ç©ºç™½å’Œä½èƒ½é‡ç‰‡æ®µ"""
        try:
            # 1. ä½¿ç”¨ librosa çš„ trim åŠŸèƒ½å»é™¤é–‹é ­å’Œçµå°¾çš„éœéŸ³
            y_trimmed, _ = librosa.effects.trim(y, top_db=25)
            
            # 2. æª¢æ¸¬å…§éƒ¨çš„éœéŸ³ç‰‡æ®µ
            # è¨ˆç®—çŸ­æ™‚èƒ½é‡
            frame_length = 2048
            hop_length = 512
            
            # åˆ†å¹€è¨ˆç®—èƒ½é‡
            frames = librosa.util.frame(y_trimmed, frame_length=frame_length, 
                                      hop_length=hop_length)
            energy = np.sum(frames ** 2, axis=0)
            
            # è¨ˆç®—å‹•æ…‹é–¾å€¼
            energy_mean = np.mean(energy)
            energy_std = np.std(energy)
            threshold = energy_mean - 2 * energy_std
            
            # æ¨™è¨˜æœ‰æ•ˆéŸ³é »ç‰‡æ®µ
            valid_frames = energy > threshold
            
            # å¹³æ»‘è™•ç†ï¼Œé¿å…éåº¦åˆ‡å‰²
            kernel = np.ones(10) / 10
            valid_frames_smooth = np.convolve(valid_frames.astype(float), kernel, mode='same') > 0.3
            
            # é‡å»ºéŸ³é »
            y_processed = []
            for i, is_valid in enumerate(valid_frames_smooth):
                if is_valid:
                    start_sample = i * hop_length
                    end_sample = min(start_sample + hop_length, len(y_trimmed))
                    y_processed.extend(y_trimmed[start_sample:end_sample])
            
            y_result = np.array(y_processed)
            
            print(f"âœ“ ç©ºç™½ç‰‡æ®µå»é™¤å®Œæˆï¼Œå¾ {len(y)/sr:.2f}ç§’ ç¸®çŸ­åˆ° {len(y_result)/sr:.2f}ç§’")
            return y_result
            
        except Exception as e:
            print(f"âœ— ç©ºç™½ç‰‡æ®µå»é™¤å¤±æ•—: {e}")
            return y
    
    def apply_noise_filters(self, y, sr):
        """æ‡‰ç”¨å¤šç¨®å™ªéŸ³æ¿¾æ³¢å™¨"""
        try:
            nyquist = sr / 2
            y_filtered = y.copy()
            
            # 1. é«˜é€šæ¿¾æ³¢å™¨ - å»é™¤ä½é »å™ªéŸ³ï¼ˆå¦‚ç©ºèª¿è²ï¼‰
            if nyquist > 100:
                sos_high = signal.butter(6, 100, btype='high', fs=sr, output='sos')
                y_filtered = signal.sosfilt(sos_high, y_filtered)
            
            # 2. å¸¶é˜»æ¿¾æ³¢å™¨ - å»é™¤é›»æºå™ªéŸ³
            power_freqs = [50, 60, 100, 120]  # é›»æºå™ªéŸ³é »ç‡
            for freq in power_freqs:
                if freq + 2 < nyquist:
                    sos_notch = signal.butter(4, [freq-2, freq+2], 
                                            btype='bandstop', fs=sr, output='sos')
                    y_filtered = signal.sosfilt(sos_notch, y_filtered)
            
            # 3. ä½é€šæ¿¾æ³¢å™¨ - å»é™¤è¶…é«˜é »å™ªéŸ³
            cutoff_freq = min(8000, nyquist * 0.9)
            sos_low = signal.butter(6, cutoff_freq, btype='low', fs=sr, output='sos')
            y_filtered = signal.sosfilt(sos_low, y_filtered)
            
            print("âœ“ å™ªéŸ³æ¿¾æ³¢å®Œæˆ")
            return y_filtered
            
        except Exception as e:
            print(f"âœ— å™ªéŸ³æ¿¾æ³¢å¤±æ•—: {e}")
            return y
    
    def extract_voice_features(self, y, sr, frame_length=2048, hop_length=512):
        """æå–èªéŸ³ç‰¹å¾µç”¨æ–¼èªªè©±è€…åˆ†é›¢"""
        try:
            # æå– MFCC ç‰¹å¾µ
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, 
                                       n_fft=frame_length, hop_length=hop_length)
            
            # æå–åŸºé » (F0)
            f0, voiced_flag, voiced_probs = librosa.pyin(y, 
                                                       fmin=librosa.note_to_hz('C2'), 
                                                       fmax=librosa.note_to_hz('C7'),
                                                       frame_length=frame_length)
            
            # æå–é »è­œé‡å¿ƒ
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr, 
                                                                 hop_length=hop_length)[0]
            
            # æå–é »è­œæ»¾é™
            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, 
                                                              hop_length=hop_length)[0]
            
            # æå–é›¶äº¤å‰ç‡
            zcr = librosa.feature.zero_crossing_rate(y, frame_length=frame_length, 
                                                   hop_length=hop_length)[0]
            
            print("âœ“ èªéŸ³ç‰¹å¾µæå–å®Œæˆ")
            return {
                'mfccs': mfccs,
                'f0': f0,
                'voiced_flag': voiced_flag,
                'spectral_centroids': spectral_centroids,
                'spectral_rolloff': spectral_rolloff,
                'zcr': zcr
            }
            
        except Exception as e:
            print(f"âœ— ç‰¹å¾µæå–å¤±æ•—: {e}")
            return None
    
    def detect_voice_activity(self, y, sr, frame_length=2048, hop_length=512):
        """æª¢æ¸¬èªéŸ³æ´»å‹•å€åŸŸ"""
        try:
            # è¨ˆç®—çŸ­æ™‚èƒ½é‡
            frame_length_samples = frame_length
            hop_length_samples = hop_length
            
            # åˆ†å¹€
            frames = librosa.util.frame(y, frame_length=frame_length_samples, 
                                      hop_length=hop_length_samples)
            
            # è¨ˆç®—æ¯å¹€çš„èƒ½é‡
            energy = np.sum(frames ** 2, axis=0)
            
            # è¨ˆç®—èƒ½é‡é–¾å€¼
            energy_threshold = np.percentile(energy, 30)  # ä½¿ç”¨30%åˆ†ä½æ•¸ä½œç‚ºé–¾å€¼
            
            # æª¢æ¸¬èªéŸ³æ´»å‹•
            voice_activity = energy > energy_threshold
            
            # å¹³æ»‘è™•ç†
            kernel = np.ones(5) / 5
            voice_activity_smooth = np.convolve(voice_activity.astype(float), kernel, mode='same') > 0.5
            
            print("âœ“ èªéŸ³æ´»å‹•æª¢æ¸¬å®Œæˆ")
            return voice_activity_smooth, energy
            
        except Exception as e:
            print(f"âœ— èªéŸ³æ´»å‹•æª¢æ¸¬å¤±æ•—: {e}")
            return None, None
    
    def separate_speakers(self, y, sr, n_speakers=2):
        """åˆ†é›¢ä¸åŒèªªè©±è€…çš„èªéŸ³"""
        try:
            print(f"é–‹å§‹åˆ†é›¢ {n_speakers} å€‹èªªè©±è€…...")
            
            # åƒæ•¸è¨­å®š
            frame_length = 2048
            hop_length = 512
            
            # æª¢æ¸¬èªéŸ³æ´»å‹•
            voice_activity, energy = self.detect_voice_activity(y, sr, frame_length, hop_length)
            if voice_activity is None:
                return None
            
            # æå–ç‰¹å¾µ
            features = self.extract_voice_features(y, sr, frame_length, hop_length)
            if features is None:
                return None
            
            # åªåœ¨æœ‰èªéŸ³æ´»å‹•çš„å¹€ä¸Šé€²è¡Œåˆ†æ
            voiced_frames = voice_activity
            
            # æº–å‚™èšé¡ç‰¹å¾µ
            feature_matrix = []
            valid_indices = []
            
            for i in range(len(voiced_frames)):
                if voiced_frames[i] and i < features['mfccs'].shape[1]:
                    frame_features = []
                    
                    # MFCC ç‰¹å¾µ
                    frame_features.extend(features['mfccs'][:, i])
                    
                    # åŸºé »ç‰¹å¾µ (å¦‚æœæœ‰æ•ˆ)
                    if i < len(features['f0']) and not np.isnan(features['f0'][i]):
                        frame_features.append(features['f0'][i])
                    else:
                        frame_features.append(0)
                    
                    # å…¶ä»–ç‰¹å¾µ
                    if i < len(features['spectral_centroids']):
                        frame_features.append(features['spectral_centroids'][i])
                    if i < len(features['spectral_rolloff']):
                        frame_features.append(features['spectral_rolloff'][i])
                    if i < len(features['zcr']):
                        frame_features.append(features['zcr'][i])
                    
                    feature_matrix.append(frame_features)
                    valid_indices.append(i)
            
            if len(feature_matrix) < n_speakers:
                print("âœ— èªéŸ³å¹€æ•¸ä¸è¶³ä»¥é€²è¡Œèªªè©±è€…åˆ†é›¢")
                return None
            
            # æ¨™æº–åŒ–ç‰¹å¾µ
            feature_matrix = np.array(feature_matrix)
            scaler = StandardScaler()
            feature_matrix_scaled = scaler.fit_transform(feature_matrix)
            
            # K-means èšé¡
            kmeans = KMeans(n_clusters=n_speakers, random_state=42, n_init=10)
            speaker_labels = kmeans.fit_predict(feature_matrix_scaled)
            
            print(f"âœ“ èªªè©±è€…åˆ†é›¢å®Œæˆï¼Œè­˜åˆ¥å‡º {len(np.unique(speaker_labels))} å€‹èªªè©±è€…")
            
            return {
                'speaker_labels': speaker_labels,
                'valid_indices': valid_indices,
                'voice_activity': voice_activity,
                'hop_length': hop_length
            }
            
        except Exception as e:
            print(f"âœ— èªªè©±è€…åˆ†é›¢å¤±æ•—: {e}")
            return None    

    def create_speaker_masks(self, y, sr, separation_result):
        """ç‚ºæ¯å€‹èªªè©±è€…å‰µå»ºéŸ³é »é®ç½©"""
        try:
            speaker_labels = separation_result['speaker_labels']
            valid_indices = separation_result['valid_indices']
            voice_activity = separation_result['voice_activity']
            hop_length = separation_result['hop_length']
            
            # å‰µå»ºæ™‚é–“è»¸å°æ‡‰
            n_frames = len(voice_activity)
            n_speakers = len(np.unique(speaker_labels))
            
            # åˆå§‹åŒ–èªªè©±è€…é®ç½©
            speaker_masks = np.zeros((n_speakers, n_frames))
            
            # å¡«å……èªªè©±è€…æ¨™ç±¤
            label_idx = 0
            for frame_idx in range(n_frames):
                if voice_activity[frame_idx] and label_idx < len(speaker_labels):
                    speaker_id = speaker_labels[label_idx]
                    speaker_masks[speaker_id, frame_idx] = 1.0
                    label_idx += 1
            
            # å¹³æ»‘é®ç½©
            for speaker_id in range(n_speakers):
                kernel = np.ones(5) / 5
                speaker_masks[speaker_id] = np.convolve(speaker_masks[speaker_id], kernel, mode='same')
            
            print(f"âœ“ å‰µå»ºäº† {n_speakers} å€‹èªªè©±è€…é®ç½©")
            return speaker_masks, hop_length
            
        except Exception as e:
            print(f"âœ— å‰µå»ºèªªè©±è€…é®ç½©å¤±æ•—: {e}")
            return None, None
    
    def apply_speaker_separation(self, y, sr, speaker_masks, hop_length):
        """æ‡‰ç”¨èªªè©±è€…é®ç½©åˆ†é›¢éŸ³é »"""
        try:
            # è¨ˆç®— STFT
            D = librosa.stft(y, hop_length=hop_length)
            magnitude, phase = np.abs(D), np.angle(D)
            
            separated_audios = []
            n_speakers = speaker_masks.shape[0]
            
            for speaker_id in range(n_speakers):
                # èª¿æ•´é®ç½©å¤§å°ä»¥åŒ¹é… STFT
                mask = speaker_masks[speaker_id]
                if len(mask) != magnitude.shape[1]:
                    # é‡æ–°æ¡æ¨£é®ç½©ä»¥åŒ¹é… STFT å¹€æ•¸
                    mask = np.interp(np.linspace(0, len(mask)-1, magnitude.shape[1]), 
                                   np.arange(len(mask)), mask)
                
                # æ‡‰ç”¨é®ç½©
                masked_magnitude = magnitude * mask[np.newaxis, :]
                
                # é‡å»ºéŸ³é »
                masked_stft = masked_magnitude * np.exp(1j * phase)
                separated_audio = librosa.istft(masked_stft, hop_length=hop_length, length=len(y))
                
                separated_audios.append(separated_audio)
            
            print(f"âœ“ åˆ†é›¢å‡º {len(separated_audios)} å€‹èªªè©±è€…çš„éŸ³é »")
            return separated_audios
            
        except Exception as e:
            print(f"âœ— éŸ³é »åˆ†é›¢å¤±æ•—: {e}")
            return None
    
    def enhance_speech_clarity(self, y, sr):
        """å¢å¼·èªéŸ³æ¸…æ™°åº¦ï¼Œé¿å…æ¨¡ç³Š"""
        try:
            # 1. è¼•å¾®çš„é åŠ é‡
            pre_emphasis = 0.97
            y_preemph = np.append(y[0], y[1:] - pre_emphasis * y[:-1])
            
            # 2. å‹•æ…‹ç¯„åœå£“ç¸®ï¼ˆè¼•å¾®ï¼‰
            y_compressed = np.sign(y_preemph) * np.power(np.abs(y_preemph), 0.8)
            
            # 3. é«˜é »å¢å¼·ï¼ˆé‡å°èªéŸ³æ¸…æ™°åº¦ï¼‰
            # è¨­è¨ˆä¸€å€‹è¼•å¾®çš„é«˜é »å¢å¼·æ¿¾æ³¢å™¨
            nyquist = sr / 2
            high_freq = min(4000, nyquist * 0.8)  # å¢å¼· 4kHz ä»¥ä¸‹çš„é«˜é »
            
            # å‰µå»ºä¸€å€‹è¼•å¾®çš„é«˜é »å¢å¼·æ¿¾æ³¢å™¨
            b, a = signal.butter(2, [1000, high_freq], btype='band', fs=sr)
            high_freq_component = signal.filtfilt(b, a, y_compressed)
            
            # æ··åˆåŸå§‹ä¿¡è™Ÿå’Œé«˜é »å¢å¼·
            y_enhanced = y_compressed + 0.1 * high_freq_component
            
            # 4. æ­£è¦åŒ–ï¼Œé¿å…å‰Šæ³¢
            max_val = np.max(np.abs(y_enhanced))
            if max_val > 0:
                y_enhanced = y_enhanced / max_val * 0.95
            
            print("âœ“ èªéŸ³æ¸…æ™°åº¦å¢å¼·å®Œæˆ")
            return y_enhanced
            
        except Exception as e:
            print(f"âœ— èªéŸ³å¢å¼·å¤±æ•—: {e}")
            return y
    
    def save_audio(self, audio_data, sr, filename):
        """å„²å­˜éŸ³é »æª”æ¡ˆ"""
        try:
            output_path = self.output_dir / filename
            sf.write(output_path, audio_data, sr)
            print(f"âœ“ éŸ³é »å·²å„²å­˜: {output_path}")
            return str(output_path)
        except Exception as e:
            print(f"âœ— éŸ³é »å„²å­˜å¤±æ•—: {e}")
            return None 
   
    def process_audio(self, n_speakers=2):
        """ä¸»è¦è™•ç†æµç¨‹"""
        print(f"ğŸµ é–‹å§‹é€²éšéŸ³é »è™•ç† - åˆ†é›¢ {n_speakers} å€‹èªªè©±è€…")
        print("=" * 60)
        print(f"è™•ç†æª”æ¡ˆ: {self.input_file}")
        
        # 1. è½‰æ›æ ¼å¼
        wav_file = self.convert_to_wav()
        if not wav_file:
            return False
        
        # 2. è¼‰å…¥éŸ³é »
        y, sr = self.load_audio(wav_file)
        if y is None:
            return False
        
        # 3. é€²éšå™ªéŸ³æ¸›å°‘ï¼ˆå»é™¤éˆ´è²å’Œç©ºç™½ç‰‡æ®µï¼‰
        y_clean = self.advanced_noise_reduction(y, sr)
        
        # 4. èªªè©±è€…åˆ†é›¢
        separation_result = self.separate_speakers(y_clean, sr, n_speakers)
        if separation_result is None:
            print("âš ï¸ èªªè©±è€…åˆ†é›¢å¤±æ•—ï¼Œå°‡ä¿å­˜æ•´é«”å»å™ªéŸ³é »")
            enhanced_audio = self.enhance_speech_clarity(y_clean, sr)
            self.save_audio(y, sr, "01_original.wav")
            self.save_audio(enhanced_audio, sr, "02_enhanced_speech.wav")
            return True
        
        # 5. å‰µå»ºèªªè©±è€…é®ç½©
        speaker_masks, hop_length = self.create_speaker_masks(y_clean, sr, separation_result)
        if speaker_masks is None:
            return False
        
        # 6. åˆ†é›¢èªªè©±è€…éŸ³é »
        separated_audios = self.apply_speaker_separation(y_clean, sr, speaker_masks, hop_length)
        if separated_audios is None:
            return False
        
        # 7. å¢å¼·æ¯å€‹èªªè©±è€…çš„èªéŸ³æ¸…æ™°åº¦
        enhanced_speakers = []
        for i, speaker_audio in enumerate(separated_audios):
            enhanced = self.enhance_speech_clarity(speaker_audio, sr)
            enhanced_speakers.append(enhanced)
        
        # 8. å„²å­˜çµæœ
        results = {}
        results['original'] = self.save_audio(y, sr, "01_original.wav")
        results['cleaned'] = self.save_audio(y_clean, sr, "02_noise_reduced.wav")
        
        # å„²å­˜æ¯å€‹èªªè©±è€…çš„éŸ³é »
        for i, (raw_audio, enhanced_audio) in enumerate(zip(separated_audios, enhanced_speakers)):
            speaker_num = i + 1
            results[f'speaker_{speaker_num}_raw'] = self.save_audio(
                raw_audio, sr, f"03_speaker_{speaker_num}_raw.wav"
            )
            results[f'speaker_{speaker_num}_enhanced'] = self.save_audio(
                enhanced_audio, sr, f"04_speaker_{speaker_num}_enhanced.wav"
            )
        
        # å‰µå»ºæ··åˆçš„å¢å¼·ç‰ˆæœ¬
        if len(enhanced_speakers) >= 2:
            mixed_enhanced = np.zeros_like(enhanced_speakers[0])
            for enhanced in enhanced_speakers:
                mixed_enhanced += enhanced
            mixed_enhanced = mixed_enhanced / len(enhanced_speakers)
            results['mixed_enhanced'] = self.save_audio(
                mixed_enhanced, sr, "05_all_speakers_enhanced.wav"
            )
        
        print("\n" + "=" * 60)
        print("ğŸ‰ è™•ç†å®Œæˆï¼è¼¸å‡ºæª”æ¡ˆ:")
        for key, path in results.items():
            if path:
                print(f"  ğŸ“ {key}: {Path(path).name}")
        
        print(f"\nğŸ’¡ å»ºè­°ä½¿ç”¨:")
        print(f"  ğŸ¯ èªªè©±è€…1: 04_speaker_1_enhanced.wav")
        print(f"  ğŸ¯ èªªè©±è€…2: 04_speaker_2_enhanced.wav")
        print(f"  ğŸ¯ æ•´é«”å¢å¼·: 05_all_speakers_enhanced.wav")
        
        return True

def main():
    parser = argparse.ArgumentParser(description='é€²éšéŸ³é »äººè²åˆ†é›¢å’Œèªªè©±è€…åˆ†é›¢å·¥å…·')
    parser.add_argument('input_file', help='è¼¸å…¥éŸ³é »æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--speakers', type=int, default=2, help='èªªè©±è€…æ•¸é‡ (é è¨­: 2)')
    parser.add_argument('--output-dir', help='è¼¸å‡ºç›®éŒ„ (å¯é¸)')
    
    args = parser.parse_args()
    
    # æª¢æŸ¥è¼¸å…¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.input_file):
        print(f"âœ— æª”æ¡ˆä¸å­˜åœ¨: {args.input_file}")
        sys.exit(1)
    
    # å‰µå»ºè™•ç†å™¨
    separator = AdvancedVoiceSeparator(args.input_file)
    
    # å¦‚æœæŒ‡å®šäº†è¼¸å‡ºç›®éŒ„ï¼Œå‰‡ä½¿ç”¨å®ƒ
    if args.output_dir:
        separator.output_dir = Path(args.output_dir)
        separator.output_dir.mkdir(exist_ok=True)
    
    # è™•ç†éŸ³é »
    success = separator.process_audio(n_speakers=args.speakers)
    
    if success:
        print("\nğŸ‰ éŸ³é »è™•ç†æˆåŠŸå®Œæˆï¼")
    else:
        print("\nâŒ éŸ³é »è™•ç†å¤±æ•—")
        sys.exit(1)

if __name__ == "__main__":
    # å¦‚æœç›´æ¥é‹è¡Œï¼Œè™•ç†æŒ‡å®šçš„æª”æ¡ˆ
    if len(sys.argv) == 1:
        # é è¨­è™•ç†æŒ‡å®šçš„æª”æ¡ˆ
        input_file = r"D:\python\Flask-AICares\TTS\03041966.m4a"
        if os.path.exists(input_file):
            separator = AdvancedVoiceSeparator(input_file)
            separator.process_audio(n_speakers=2)
        else:
            print(f"âœ— é è¨­æª”æ¡ˆä¸å­˜åœ¨: {input_file}")
            print("ä½¿ç”¨æ–¹æ³•: python advanced_voice_separation.py <éŸ³é »æª”æ¡ˆè·¯å¾‘> [--speakers 2]")
    else:
        main()