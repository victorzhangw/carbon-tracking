# AI å®¢æœç³»çµ± - æ¨¡å‹å„²å­˜èˆ‡éƒ¨ç½²å®Œæ•´æŒ‡å—

## ğŸ“‹ æ–‡ä»¶æ¦‚è¿°

æœ¬æŒ‡å—æä¾› AI å®¢æœèªéŸ³å…‹éš†ç³»çµ±çš„å®Œæ•´æ¨¡å‹å„²å­˜èˆ‡éƒ¨ç½²è§£æ±ºæ–¹æ¡ˆï¼Œæ¶µè“‹æœ¬åœ°ç«¯éƒ¨ç½²ã€é›²ç«¯éƒ¨ç½²ã€æ··åˆæ¶æ§‹ç­‰å¤šç¨®æ–¹æ¡ˆã€‚

## ğŸ¯ ç›®æ¨™è®€è€…

- **ç³»çµ±ç®¡ç†å“¡**: è² è²¬ç³»çµ±éƒ¨ç½²å’Œç¶­è­·
- **DevOps å·¥ç¨‹å¸«**: è² è²¬è‡ªå‹•åŒ–éƒ¨ç½²å’Œç›£æ§
- **æŠ€è¡“ä¸»ç®¡**: éœ€è¦äº†è§£æŠ€è¡“æ¶æ§‹å’Œæ±ºç­–ä¾æ“š
- **IT æ±ºç­–è€…**: éœ€è¦è©•ä¼°ä¸åŒéƒ¨ç½²æ–¹æ¡ˆçš„æˆæœ¬æ•ˆç›Š

---

## ğŸ—ï¸ æ¨¡å‹å„²å­˜æ¶æ§‹è¨­è¨ˆ

### æ¨¡å‹åˆ†é¡èˆ‡ç‰¹æ€§

#### 1. æƒ…ç·’è­˜åˆ¥æ¨¡å‹

- **åŸºç¤ç‰ˆæœ¬**: scikit-learn æ¨¡å‹ï¼Œæª”æ¡ˆå¤§å° ~50MB
- **é€²éšç‰ˆæœ¬**: Wav2Vec2 æ¨¡å‹ï¼Œæª”æ¡ˆå¤§å° ~1.2GB
- **æ›´æ–°é »ç‡**: æ¯æœˆä¸€æ¬¡æ¨¡å‹å„ªåŒ–
- **è¼‰å…¥æ™‚é–“**: åŸºç¤ç‰ˆ 2 ç§’ï¼Œé€²éšç‰ˆ 15 ç§’

#### 2. èªéŸ³å…‹éš†æ¨¡å‹

- **GPT æ¨¡å‹**: èªç¾©ç†è§£ï¼Œæª”æ¡ˆå¤§å° ~800MB
- **SoVITS æ¨¡å‹**: è²å­¸åˆæˆï¼Œæª”æ¡ˆå¤§å° ~600MB
- **åƒè€ƒéŸ³é »**: æ¯ä½å®¢æœ 20-50 å€‹æ¨£æœ¬ï¼Œç¸½è¨ˆ ~100MB
- **æ›´æ–°é »ç‡**: æ–°å¢å®¢æœæ™‚è¨“ç·´ï¼Œæ¯å­£åº¦å¾®èª¿

#### 3. é è¨“ç·´æ¨¡å‹

- **åŸºç¤é è¨“ç·´æ¨¡å‹**: 3 å€‹æª”æ¡ˆï¼Œç¸½è¨ˆ ~2.5GB
- **æ›´æ–°é »ç‡**: æ¯åŠå¹´æ›´æ–°ä¸€æ¬¡
- **å…±äº«æ€§**: æ‰€æœ‰å®¢æœå…±ç”¨åŸºç¤æ¨¡å‹

#

## å„²å­˜éœ€æ±‚åˆ†æ

```
ç¸½å„²å­˜éœ€æ±‚ä¼°ç®— (10ä½å®¢æœå°ˆå“¡):
â”œâ”€â”€ é è¨“ç·´æ¨¡å‹: 2.5GB (å…±ç”¨)
â”œâ”€â”€ æƒ…ç·’è­˜åˆ¥æ¨¡å‹: 1.25GB (å…±ç”¨)
â”œâ”€â”€ å€‹äººåŒ–èªéŸ³æ¨¡å‹: 14GB (10 Ã— 1.4GB)
â”œâ”€â”€ è¨“ç·´è³‡æ–™: 5GB (éŸ³é »æ¨£æœ¬)
â”œâ”€â”€ å¿«å–å’Œæ—¥èªŒ: 2GB
â””â”€â”€ ç¸½è¨ˆ: ~25GB
```

---

## ğŸ’» æœ¬åœ°ç«¯éƒ¨ç½²æ–¹æ¡ˆ

### æ¶æ§‹å„ªå‹¢

- **å®Œå…¨æ§åˆ¶**: æ•¸æ“šå’Œæ¨¡å‹å®Œå…¨åœ¨æœ¬åœ°
- **ä½å»¶é²**: ç„¡ç¶²è·¯å‚³è¼¸å»¶é²
- **æˆæœ¬å¯æ§**: ç„¡é›²ç«¯æœå‹™è²»ç”¨
- **éš±ç§ä¿è­·**: æ•æ„Ÿæ•¸æ“šä¸é›¢é–‹æœ¬åœ°ç’°å¢ƒ

### ç¡¬é«”éœ€æ±‚

#### æœ€ä½é…ç½®

- **CPU**: Intel i5-8400 æˆ– AMD Ryzen 5 2600
- **è¨˜æ†¶é«”**: 16GB DDR4
- **å„²å­˜**: 500GB SSD
- **GPU**: NVIDIA GTX 1060 6GB (å¯é¸)

#### æ¨è–¦é…ç½®

- **CPU**: Intel i7-10700K æˆ– AMD Ryzen 7 3700X
- **è¨˜æ†¶é«”**: 32GB DDR4
- **å„²å­˜**: 1TB NVMe SSD
- **GPU**: NVIDIA RTX 3070 8GB

#### ä¼æ¥­ç´šé…ç½®

- **CPU**: Intel Xeon E-2288G æˆ– AMD EPYC 7302P
- **è¨˜æ†¶é«”**: 64GB ECC DDR4
- **å„²å­˜**: 2TB NVMe SSD + 4TB HDD
- **GPU**: NVIDIA RTX A4000 16GB#

## æœ¬åœ°ç«¯å„²å­˜æ¶æ§‹

````python
# æœ¬åœ°ç«¯æ¨¡å‹ç®¡ç†ç³»çµ±
class LocalModelManager:
    def __init__(self, base_path="./models"):
        self.base_path = base_path
        self.config = {
            'model_storage': {
                'base_path': base_path,
                'max_model_size': '2GB',
                'backup_enabled': True,
                'backup_interval': '24h',
                'cleanup_old_models': True,
                'max_model_versions': 5
            },
            'cache': {
                'enabled': True,
                'max_size': '1GB',
                'ttl': 3600  # 1å°æ™‚
            }
        }
        self.setup_directories()

    def setup_directories(self):
        """åˆå§‹åŒ–ç›®éŒ„çµæ§‹"""
        directories = [
            f"{self.base_path}/emotion_recognition/basic",
            f"{self.base_path}/emotion_recognition/advanced",
            f"{self.base_path}/gpt_sovits/pretrained",
            f"{self.base_path}/gpt_sovits/fine_tuned",
            f"{self.base_path}/cache/embeddings",
            f"{self.base_path}/cache/inference_cache",
            f"{self.base_path}/backups",
            "./audio_uploads",
            "./voice_output",
            "./logs"
        ]

        for directory in directories:
            os.makedirs(directory, exist_ok=True)

    def save_emotion_model(self, model_type, model_data, metadata):
        """å„²å­˜æƒ…ç·’è­˜åˆ¥æ¨¡å‹"""
        model_dir = f"{self.base_path}/emotion_recognition/{model_type}"

        # å‰µå»ºç‰ˆæœ¬åŒ–å„²å­˜
        version = metadata.get('version', '1.0.0')
        versioned_dir = f"{model_dir}/v{version}"
        os.makedirs(versioned_dir, exist_ok=True)

        if model_type == "basic":
            # å„²å­˜ scikit-learn æ¨¡å‹
            joblib.dump(model_data['model'], f"{versioned_dir}/model.pkl")
            joblib.dump(model_data['scaler'], f"{versioned_dir}/scaler.pkl")
        elif model_type == "advanced":
            # å„²å­˜ PyTorch æ¨¡å‹
            model_data['model'].save_pretrained(versioned_dir)
            model_data['tokenizer'].save_pretrained(versioned_dir)

        # å„²å­˜å…ƒæ•¸æ“š
        metadata['saved_at'] = datetime.now().isoformat()
        metadata['file_size'] = self._get_directory_size(versioned_dir)

        with open(f"{versioned_dir}/metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        # å‰µå»ºç¬¦è™Ÿé€£çµåˆ°æœ€æ–°ç‰ˆæœ¬
        latest_link = f"{model_dir}/latest"
        if os.path.exists(latest_link):
            os.remove(latest_link)
        os.symlink(versioned_dir, latest_link)

        # æ¸…ç†èˆŠç‰ˆæœ¬
        self._cleanup_old_versions(model_dir)

        return versioned_dir
```    def
 save_voice_model(self, staff_code, model_data, metadata):
        """å„²å­˜èªéŸ³å…‹éš†æ¨¡å‹"""
        model_dir = f"{self.base_path}/gpt_sovits/fine_tuned/{staff_code}"

        # å‰µå»ºç‰ˆæœ¬åŒ–å„²å­˜
        version = metadata.get('version', '1.0.0')
        versioned_dir = f"{model_dir}/v{version}"
        os.makedirs(versioned_dir, exist_ok=True)

        # å„²å­˜æ¨¡å‹æª”æ¡ˆ
        model_files = {
            'gpt_model.ckpt': model_data['gpt_model'],
            'sovits_model.pth': model_data['sovits_model'],
            'reference_audio.wav': model_data['reference_audio']
        }

        for filename, source_path in model_files.items():
            target_path = f"{versioned_dir}/{filename}"
            shutil.copy2(source_path, target_path)

        # å„²å­˜é…ç½®å’Œå…ƒæ•¸æ“š
        config_data = {
            'staff_code': staff_code,
            'model_config': model_data.get('config', {}),
            'training_params': model_data.get('training_params', {}),
            'performance_metrics': model_data.get('metrics', {})
        }

        with open(f"{versioned_dir}/config.json", 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)

        # æ›´æ–°å…ƒæ•¸æ“š
        metadata.update({
            'saved_at': datetime.now().isoformat(),
            'file_size': self._get_directory_size(versioned_dir),
            'model_files': list(model_files.keys())
        })

        with open(f"{versioned_dir}/metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        # å‰µå»ºç¬¦è™Ÿé€£çµåˆ°æœ€æ–°ç‰ˆæœ¬
        latest_link = f"{model_dir}/latest"
        if os.path.exists(latest_link):
            os.remove(latest_link)
        os.symlink(versioned_dir, latest_link)

        # æ›´æ–°è³‡æ–™åº«è¨˜éŒ„
        self._update_model_database_record(staff_code, versioned_dir, metadata)

        return versioned_dir

    def load_voice_model(self, staff_code, version='latest'):
        """è¼‰å…¥èªéŸ³å…‹éš†æ¨¡å‹"""
        if version == 'latest':
            model_dir = f"{self.base_path}/gpt_sovits/fine_tuned/{staff_code}/latest"
        else:
            model_dir = f"{self.base_path}/gpt_sovits/fine_tuned/{staff_code}/v{version}"

        if not os.path.exists(model_dir):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° {staff_code} çš„èªéŸ³æ¨¡å‹ç‰ˆæœ¬ {version}")

        # è¼‰å…¥æ¨¡å‹æª”æ¡ˆè·¯å¾‘
        model_data = {
            'gpt_model': f"{model_dir}/gpt_model.ckpt",
            'sovits_model': f"{model_dir}/sovits_model.pth",
            'reference_audio': f"{model_dir}/reference_audio.wav"
        }

        # è¼‰å…¥é…ç½®å’Œå…ƒæ•¸æ“š
        with open(f"{model_dir}/config.json", 'r', encoding='utf-8') as f:
            model_data['config'] = json.load(f)

        with open(f"{model_dir}/metadata.json", 'r', encoding='utf-8') as f:
            model_data['metadata'] = json.load(f)

        return model_data
   def _cleanup_old_versions(self, model_dir):
        """æ¸…ç†èˆŠç‰ˆæœ¬æ¨¡å‹"""
        max_versions = self.config['model_storage']['max_model_versions']

        # ç²å–æ‰€æœ‰ç‰ˆæœ¬ç›®éŒ„
        version_dirs = []
        for item in os.listdir(model_dir):
            if item.startswith('v') and os.path.isdir(f"{model_dir}/{item}"):
                version_dirs.append(item)

        # æŒ‰ç‰ˆæœ¬è™Ÿæ’åº
        version_dirs.sort(key=lambda x: version.parse(x[1:]), reverse=True)

        # åˆªé™¤è¶…å‡ºé™åˆ¶çš„èˆŠç‰ˆæœ¬
        for old_version in version_dirs[max_versions:]:
            old_path = f"{model_dir}/{old_version}"
            shutil.rmtree(old_path)
            print(f"å·²åˆªé™¤èˆŠç‰ˆæœ¬: {old_path}")

    def _get_directory_size(self, directory):
        """è¨ˆç®—ç›®éŒ„å¤§å°"""
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(directory):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                total_size += os.path.getsize(filepath)
        return total_size

    def backup_models(self):
        """å‚™ä»½æ‰€æœ‰æ¨¡å‹"""
        backup_dir = f"{self.base_path}/backups/{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(backup_dir, exist_ok=True)

        # å‚™ä»½æ¨¡å‹ç›®éŒ„
        shutil.copytree(f"{self.base_path}/emotion_recognition",
                       f"{backup_dir}/emotion_recognition")
        shutil.copytree(f"{self.base_path}/gpt_sovits",
                       f"{backup_dir}/gpt_sovits")

        # å‰µå»ºå‚™ä»½æ¸…å–®
        backup_info = {
            'backup_time': datetime.now().isoformat(),
            'backup_size': self._get_directory_size(backup_dir),
            'models_included': self.list_all_models()
        }

        with open(f"{backup_dir}/backup_info.json", 'w', encoding='utf-8') as f:
            json.dump(backup_info, f, indent=2, ensure_ascii=False)

        return backup_dir

    def restore_from_backup(self, backup_path):
        """å¾å‚™ä»½æ¢å¾©æ¨¡å‹"""
        if not os.path.exists(backup_path):
            raise FileNotFoundError(f"å‚™ä»½è·¯å¾‘ä¸å­˜åœ¨: {backup_path}")

        # å‰µå»ºç•¶å‰ç‹€æ…‹çš„å‚™ä»½
        current_backup = self.backup_models()
        print(f"å·²å‰µå»ºç•¶å‰ç‹€æ…‹å‚™ä»½: {current_backup}")

        try:
            # æ¢å¾©æ¨¡å‹
            if os.path.exists(f"{backup_path}/emotion_recognition"):
                shutil.rmtree(f"{self.base_path}/emotion_recognition")
                shutil.copytree(f"{backup_path}/emotion_recognition",
                               f"{self.base_path}/emotion_recognition")

            if os.path.exists(f"{backup_path}/gpt_sovits"):
                shutil.rmtree(f"{self.base_path}/gpt_sovits")
                shutil.copytree(f"{backup_path}/gpt_sovits",
                               f"{self.base_path}/gpt_sovits")

            print(f"å·²æˆåŠŸå¾å‚™ä»½æ¢å¾©: {backup_path}")

        except Exception as e:
            print(f"æ¢å¾©å¤±æ•—ï¼Œæ­£åœ¨å›æ»¾: {e}")
            # å›æ»¾åˆ°æ¢å¾©å‰çš„ç‹€æ…‹
            self.restore_from_backup(current_backup)
            raise###
 æœ¬åœ°ç«¯éƒ¨ç½²é…ç½®

```python
# æœ¬åœ°éƒ¨ç½²é…ç½®ç®¡ç†
class LocalDeploymentConfig:
    def __init__(self):
        self.config = {
            # æ¨¡å‹å„²å­˜é…ç½®
            'model_storage': {
                'base_path': './models',
                'max_model_size': '2GB',
                'backup_enabled': True,
                'backup_interval': '24h',
                'cleanup_old_models': True,
                'max_model_versions': 5,
                'compression_enabled': True,
                'encryption_enabled': False
            },

            # æ¨ç†æœå‹™é…ç½®
            'inference': {
                'max_concurrent_requests': 4,
                'model_cache_size': '1GB',
                'gpu_memory_fraction': 0.8,
                'cpu_threads': 4,
                'timeout': 60,
                'batch_size': 1,
                'warm_up_enabled': True
            },

            # æª”æ¡ˆå„²å­˜é…ç½®
            'file_storage': {
                'audio_upload_path': './audio_uploads',
                'voice_output_path': './voice_output',
                'temp_path': './temp',
                'max_file_size': '50MB',
                'allowed_formats': ['wav', 'mp3', 'm4a', 'flac'],
                'auto_cleanup': True,
                'cleanup_interval': '7d'
            },

            # ç›£æ§é…ç½®
            'monitoring': {
                'enabled': True,
                'log_level': 'INFO',
                'metrics_collection': True,
                'performance_tracking': True,
                'error_reporting': True
            }
        }

    def get_model_path(self, model_type, staff_code=None, version='latest'):
        """ç²å–æ¨¡å‹è·¯å¾‘"""
        base_path = self.config['model_storage']['base_path']

        if model_type == 'emotion':
            return f"{base_path}/emotion_recognition/{version}"
        elif model_type == 'voice' and staff_code:
            return f"{base_path}/gpt_sovits/fine_tuned/{staff_code}/{version}"
        elif model_type == 'pretrained':
            return f"{base_path}/gpt_sovits/pretrained"
        else:
            return base_path

    def validate_hardware_requirements(self):
        """é©—è­‰ç¡¬é«”éœ€æ±‚"""
        import psutil
        import GPUtil

        requirements = {
            'cpu_cores': 4,
            'memory_gb': 16,
            'disk_space_gb': 100,
            'gpu_memory_gb': 6
        }

        # æª¢æŸ¥ CPU
        cpu_cores = psutil.cpu_count(logical=False)
        if cpu_cores < requirements['cpu_cores']:
            print(f"è­¦å‘Š: CPU æ ¸å¿ƒæ•¸ä¸è¶³ ({cpu_cores} < {requirements['cpu_cores']})")

        # æª¢æŸ¥è¨˜æ†¶é«”
        memory_gb = psutil.virtual_memory().total / (1024**3)
        if memory_gb < requirements['memory_gb']:
            print(f"è­¦å‘Š: è¨˜æ†¶é«”ä¸è¶³ ({memory_gb:.1f}GB < {requirements['memory_gb']}GB)")

        # æª¢æŸ¥ç£ç¢Ÿç©ºé–“
        disk_space_gb = psutil.disk_usage('.').free / (1024**3)
        if disk_space_gb < requirements['disk_space_gb']:
            print(f"è­¦å‘Š: ç£ç¢Ÿç©ºé–“ä¸è¶³ ({disk_space_gb:.1f}GB < {requirements['disk_space_gb']}GB)")

        # æª¢æŸ¥ GPU
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu_memory_gb = gpus[0].memoryTotal / 1024
                if gpu_memory_gb < requirements['gpu_memory_gb']:
                    print(f"è­¦å‘Š: GPU è¨˜æ†¶é«”ä¸è¶³ ({gpu_memory_gb:.1f}GB < {requirements['gpu_memory_gb']}GB)")
            else:
                print("è­¦å‘Š: æœªæª¢æ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU é€²è¡Œæ¨ç†")
        except:
            print("ç„¡æ³•æª¢æ¸¬ GPU ç‹€æ…‹")

        return True
```###
æœ¬åœ°ç«¯å®‰è£éƒ¨ç½²æ­¥é©Ÿ

#### 1. ç’°å¢ƒæº–å‚™

```bash
# 1. æª¢æŸ¥ Python ç‰ˆæœ¬
python --version  # éœ€è¦ Python 3.10+

# 2. å‰µå»ºè™›æ“¬ç’°å¢ƒ
python -m venv ai_customer_service_env
source ai_customer_service_env/bin/activate  # Linux/Mac
# æˆ–
ai_customer_service_env\Scripts\activate  # Windows

# 3. å‡ç´š pip
python -m pip install --upgrade pip

# 4. å®‰è£ CUDA (å¦‚æœæœ‰ NVIDIA GPU)
# ä¸‹è¼‰ä¸¦å®‰è£ CUDA 11.8 æˆ– 12.1
# https://developer.nvidia.com/cuda-downloads
````

#### 2. ç³»çµ±å®‰è£

```bash
# 1. å…‹éš†å°ˆæ¡ˆ
git clone https://github.com/your-repo/ai-customer-service.git
cd ai-customer-service

# 2. å®‰è£å¾Œç«¯ä¾è³´
pip install -r requirements.txt

# 3. å®‰è£é¡å¤–çš„ GPU æ”¯æ´ (å¯é¸)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 4. åˆå§‹åŒ–è³‡æ–™åº«
python database_emotion_extension.py

# 5. ä¸‹è¼‰é è¨“ç·´æ¨¡å‹
python scripts/download_pretrained_models.py

# 6. å®‰è£å‰ç«¯ä¾è³´
cd webpage/ai-customer-service-frontend
npm install
npm run build
cd ../..
```

#### 3. é…ç½®è¨­å®š

```python
# config/local_config.py
LOCAL_CONFIG = {
    'database': {
        'url': 'sqlite:///ai_customer_service.db',
        'echo': False
    },
    'models': {
        'base_path': './models',
        'cache_enabled': True,
        'gpu_enabled': True,
        'batch_size': 1
    },
    'storage': {
        'audio_upload_path': './audio_uploads',
        'voice_output_path': './voice_output',
        'max_file_size': 50 * 1024 * 1024  # 50MB
    },
    'security': {
        'jwt_secret_key': 'your-secret-key-here',
        'jwt_expiration_hours': 24
    }
}
```

#### 4. å•Ÿå‹•æœå‹™

```bash
# 1. å•Ÿå‹•å¾Œç«¯æœå‹™
python app.py

# 2. é©—è­‰æœå‹™ç‹€æ…‹
curl http://localhost:5000/health

# 3. æ¸¬è©¦ API ç«¯é»
curl -X POST http://localhost:5000/api/emotion/analyze \
  -H "Content-Type: application/json" \
  -d '{"text": "æ¸¬è©¦æƒ…ç·’åˆ†æ"}'
```

#### 5. ç³»çµ±é©—è­‰

````python
# scripts/system_validation.py
import requests
import json

def validate_system():
    """é©—è­‰ç³»çµ±å„é …åŠŸèƒ½"""
    base_url = "http://localhost:5000"

    # 1. å¥åº·æª¢æŸ¥
    response = requests.get(f"{base_url}/health")
    assert response.status_code == 200
    print("âœ“ å¥åº·æª¢æŸ¥é€šé")

    # 2. æƒ…ç·’åˆ†ææ¸¬è©¦
    emotion_data = {"text": "æˆ‘å¾ˆé–‹å¿ƒä»Šå¤©çš„æœå‹™"}
    response = requests.post(f"{base_url}/api/emotion/analyze",
                           json=emotion_data)
    assert response.status_code == 200
    print("âœ“ æƒ…ç·’åˆ†æåŠŸèƒ½æ­£å¸¸")

    # 3. æ¨¡å‹åˆ—è¡¨æ¸¬è©¦
    response = requests.get(f"{base_url}/api/models/list")
    assert response.status_code == 200
    print("âœ“ æ¨¡å‹ç®¡ç†åŠŸèƒ½æ­£å¸¸")

    print("ç³»çµ±é©—è­‰å®Œæˆï¼")

if __name__ == "__main__":
    validate_system()
```---

##
 â˜ï¸ é›²ç«¯éƒ¨ç½²æ–¹æ¡ˆ (Google Cloud Platform)

### æ¶æ§‹å„ªå‹¢
- **ç„¡é™æ“´å±•**: æ”¯æ´å¤§é‡ä¸¦ç™¼ç”¨æˆ¶
- **é«˜å¯ç”¨æ€§**: 99.9% ä»¥ä¸Šçš„æœå‹™å¯ç”¨æ€§
- **è‡ªå‹•åŒ–ç®¡ç†**: æ¸›å°‘ç¶­è­·å·¥ä½œé‡
- **ä¼æ¥­ç´šå®‰å…¨**: Google çš„å®‰å…¨åŸºç¤è¨­æ–½
- **æˆæœ¬å„ªåŒ–**: æŒ‰éœ€ä»˜è²»ï¼Œå¯æ¶å å¯¦ä¾‹

### GCP æœå‹™æ¶æ§‹

```yaml
# GCP æœå‹™çµ„ä»¶
GCP_Services:
  Compute:
    - Google Kubernetes Engine (GKE)     # å®¹å™¨ç·¨æ’å¹³å°
    - Compute Engine (GPU instances)     # GPU é‹ç®—å¯¦ä¾‹
    - Cloud Run                          # ç„¡ä¼ºæœå™¨å®¹å™¨æœå‹™
    - Cloud Functions                    # äº‹ä»¶é©…å‹•å‡½æ•¸

  Storage:
    - Cloud Storage                      # ç‰©ä»¶å„²å­˜ (æ¨¡å‹æª”æ¡ˆ)
    - Cloud SQL (PostgreSQL)            # é—œè¯å¼è³‡æ–™åº«
    - Cloud Firestore                   # NoSQL æ–‡æª”è³‡æ–™åº«
    - Persistent Disk                    # æŒä¹…åŒ–å€å¡Šå„²å­˜
    - Cloud Memorystore                  # Redis å¿«å–æœå‹™

  AI_ML:
    - Vertex AI                          # çµ±ä¸€æ©Ÿå™¨å­¸ç¿’å¹³å°
    - AI Platform Prediction            # æ¨¡å‹æ¨ç†æœå‹™
    - AutoML                             # è‡ªå‹•æ©Ÿå™¨å­¸ç¿’
    - Speech-to-Text API                 # èªéŸ³è­˜åˆ¥æœå‹™
    - Text-to-Speech API                 # èªéŸ³åˆæˆæœå‹™

  Networking:
    - Cloud Load Balancer                # è² è¼‰å‡è¡¡å™¨
    - Cloud CDN                          # å…§å®¹åˆ†ç™¼ç¶²è·¯
    - VPC                                # è™›æ“¬ç§æœ‰é›²
    - Cloud NAT                          # ç¶²è·¯ä½å€è½‰æ›

  Security:
    - Identity and Access Management     # èº«ä»½å’Œå­˜å–ç®¡ç†
    - Cloud KMS                          # é‡‘é‘°ç®¡ç†æœå‹™
    - Cloud Security Command Center     # å®‰å…¨æŒ‡æ®ä¸­å¿ƒ
    - Cloud Armor                        # DDoS é˜²è­·

  Monitoring:
    - Cloud Monitoring                   # ç³»çµ±ç›£æ§
    - Cloud Logging                      # æ—¥èªŒç®¡ç†
    - Cloud Trace                        # åˆ†æ•£å¼è¿½è¹¤
    - Cloud Profiler                     # æ€§èƒ½åˆ†æ
````

### GCP æ¨¡å‹å„²å­˜æ¶æ§‹

```python
# GCP æ¨¡å‹ç®¡ç†ç³»çµ±
from google.cloud import storage, aiplatform
from google.cloud.sql import connector
import json
import os

class GCPModelManager:
    def __init__(self, project_id, region="us-central1"):
        self.project_id = project_id
        self.region = region
        self.bucket_name = f"{project_id}-ai-models"

        # åˆå§‹åŒ– GCP å®¢æˆ¶ç«¯
        self.storage_client = storage.Client(project=project_id)
        self.bucket = self._get_or_create_bucket()

        # åˆå§‹åŒ– Vertex AI
        aiplatform.init(project=project_id, location=region)

    def _get_or_create_bucket(self):
        """ç²å–æˆ–å‰µå»º Cloud Storage bucket"""
        try:
            bucket = self.storage_client.bucket(self.bucket_name)
            bucket.reload()
            return bucket
        except:
            # å‰µå»ºæ–°çš„ bucket
            bucket = self.storage_client.create_bucket(
                self.bucket_name,
                location=self.region
            )

            # è¨­ç½®ç”Ÿå‘½é€±æœŸè¦å‰‡
            lifecycle_rule = {
                'action': {'type': 'Delete'},
                'condition': {
                    'age': 365,  # 365å¤©å¾Œåˆªé™¤
                    'matchesPrefix': ['temp/', 'cache/']
                }
            }
            bucket.lifecycle_rules = [lifecycle_rule]
            bucket.patch()

            return bucket

    def upload_model_to_gcs(self, local_path, gcs_path, metadata=None):
        """ä¸Šå‚³æ¨¡å‹åˆ° Google Cloud Storage"""
        blob = self.bucket.blob(gcs_path)

        # è¨­ç½®å…ƒæ•¸æ“š
        if metadata:
            blob.metadata = metadata

        # è¨­ç½®å¿«å–æ§åˆ¶
        blob.cache_control = "public, max-age=3600"

        # ä¸Šå‚³æª”æ¡ˆ
        blob.upload_from_filename(local_path)

        # è¨­ç½®å…¬é–‹è®€å–æ¬Šé™ (å¦‚æœéœ€è¦)
        # blob.make_public()

        print(f"æ¨¡å‹å·²ä¸Šå‚³åˆ° gs://{self.bucket_name}/{gcs_path}")
        return f"gs://{self.bucket_name}/{gcs_path}"

    def download_model_from_gcs(self, gcs_path, local_path):
        """å¾ GCS ä¸‹è¼‰æ¨¡å‹"""
        blob = self.bucket.blob(gcs_path)

        # å‰µå»ºæœ¬åœ°ç›®éŒ„
        os.makedirs(os.path.dirname(local_path), exist_ok=True)

        # ä¸‹è¼‰æª”æ¡ˆ
        blob.download_to_filename(local_path)

        print(f"æ¨¡å‹å·²ä¸‹è¼‰åˆ° {local_path}")
        return local_path

    def upload_voice_model(self, staff_code, model_data, metadata):
        """ä¸Šå‚³èªéŸ³å…‹éš†æ¨¡å‹åˆ° GCS"""
        version = metadata.get('version', '1.0.0')
        base_path = f"voice_models/{staff_code}/v{version}"

        uploaded_files = {}

        # ä¸Šå‚³æ¨¡å‹æª”æ¡ˆ
        model_files = {
            'gpt_model.ckpt': model_data['gpt_model'],
            'sovits_model.pth': model_data['sovits_model'],
            'reference_audio.wav': model_data['reference_audio']
        }

        for filename, local_path in model_files.items():
            gcs_path = f"{base_path}/{filename}"
            uploaded_files[filename] = self.upload_model_to_gcs(
                local_path, gcs_path, metadata
            )

        # ä¸Šå‚³é…ç½®æª”æ¡ˆ
        config_data = {
            'staff_code': staff_code,
            'version': version,
            'model_files': uploaded_files,
            'metadata': metadata
        }

        config_blob = self.bucket.blob(f"{base_path}/config.json")
        config_blob.upload_from_string(
            json.dumps(config_data, indent=2, ensure_ascii=False),
            content_type='application/json'
        )

        return uploaded_files    def dep
loy_model_to_vertex_ai(self, model_path, model_name, version):
        """éƒ¨ç½²æ¨¡å‹åˆ° Vertex AI"""

        # å‰µå»ºæ¨¡å‹è³‡æº
        model = aiplatform.Model.upload(
            display_name=f"{model_name}-{version}",
            artifact_uri=model_path,
            serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/pytorch-gpu.1-13:latest",
            serving_container_predict_route="/predict",
            serving_container_health_route="/health",
            description=f"AI Customer Service Model - {model_name} v{version}"
        )

        # å‰µå»ºç«¯é»
        endpoint = aiplatform.Endpoint.create(
            display_name=f"{model_name}-endpoint-{version}",
            description=f"Endpoint for {model_name} model"
        )

        # éƒ¨ç½²æ¨¡å‹åˆ°ç«¯é»
        deployed_model = model.deploy(
            endpoint=endpoint,
            deployed_model_display_name=f"{model_name}-deployed-{version}",
            machine_type="n1-standard-4",
            accelerator_type="NVIDIA_TESLA_T4",
            accelerator_count=1,
            min_replica_count=1,
            max_replica_count=10,
            traffic_percentage=100
        )

        return {
            'model': model,
            'endpoint': endpoint,
            'deployed_model': deployed_model
        }

    def create_model_version(self, model_name, model_path, version_name, description=""):
        """å‰µå»ºæ–°çš„æ¨¡å‹ç‰ˆæœ¬"""

        # ä¸Šå‚³æ¨¡å‹åˆ° Model Registry
        model = aiplatform.Model.upload(
            display_name=f"{model_name}-{version_name}",
            artifact_uri=model_path,
            serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/pytorch-gpu.1-13:latest",
            version_aliases=[version_name],
            version_description=description,
            is_default_version=True
        )

        return model

    def list_model_versions(self, model_name):
        """åˆ—å‡ºæ¨¡å‹çš„æ‰€æœ‰ç‰ˆæœ¬"""
        models = aiplatform.Model.list(
            filter=f'display_name="{model_name}"',
            order_by="create_time desc"
        )

        versions = []
        for model in models:
            versions.append({
                'name': model.display_name,
                'version': model.version_id,
                'create_time': model.create_time,
                'update_time': model.update_time,
                'description': model.version_description
            })

        return versions

    def setup_model_monitoring(self, endpoint_name):
        """è¨­ç½®æ¨¡å‹ç›£æ§"""
        from google.cloud import monitoring_v3

        client = monitoring_v3.MetricServiceClient()
        project_name = f"projects/{self.project_id}"

        # å‰µå»ºè‡ªå®šç¾©æŒ‡æ¨™
        descriptor = monitoring_v3.MetricDescriptor()
        descriptor.type = f"custom.googleapis.com/ai_service/{endpoint_name}/prediction_latency"
        descriptor.metric_kind = monitoring_v3.MetricDescriptor.MetricKind.GAUGE
        descriptor.value_type = monitoring_v3.MetricDescriptor.ValueType.DOUBLE
        descriptor.description = "AI model prediction latency"

        descriptor = client.create_metric_descriptor(
            name=project_name,
            metric_descriptor=descriptor
        )

        return descriptor
```

### Kubernetes éƒ¨ç½²é…ç½®

````yaml
# kubernetes/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ai-customer-service
  labels:
    name: ai-customer-service

---
# kubernetes/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-service-config
  namespace: ai-customer-service
data:
  DATABASE_URL: "postgresql://user:password@postgres-service:5432/ai_service"
  GCS_BUCKET: "your-project-ai-models"
  REDIS_URL: "redis://redis-service:6379"
  MODEL_CACHE_SIZE: "2Gi"
  MAX_CONCURRENT_REQUESTS: "10"

---
# kubernetes/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: ai-service-secrets
  namespace: ai-customer-service
type: Opaque
data:
  jwt-secret: <base64-encoded-jwt-secret>
  db-password: <base64-encoded-db-password>
  gcp-service-account: <base64-encoded-service-account-json>

---
# kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-customer-service-backend
  namespace: ai-customer-service
  labels:
    app: ai-customer-service
    component: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-customer-service
      component: backend
  template:
    metadata:
      labels:
        app: ai-customer-service
        component: backend
    spec:
      containers:
      - name: flask-app
        image: gcr.io/PROJECT_ID/ai-customer-service:latest
        ports:
        - containerPort: 5000
          name: http
        env:
        - name: DATABASE_URL
          valueFrom:
            configMapKeyRef:
              name: ai-service-config
              key: DATABASE_URL
        - name: GCS_BUCKET
          valueFrom:
            configMapKeyRef:
              name: ai-service-config
              key: GCS_BUCKET
        - name: JWT_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: ai-service-secrets
              key: jwt-secret
        - name: GOOGLE_APPLICATION_CREDENTIALS
          value: "/etc/gcp/service-account.json"
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
            nvidia.com/gpu: 1
          limits:
            memory: "8Gi"
            cpu: "4000m"
            nvidia.com/gpu: 1
        volumeMounts:
        - name: gcp-credentials
          mountPath: /etc/gcp
          readOnly: true
        - name: model-cache
          mountPath: /app/models
        - name: temp-storage
          mountPath: /app/temp
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 5000
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: gcp-credentials
        secret:
          secretName: ai-service-secrets
          items:
          - key: gcp-service-account
            path: service-account.json
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-cache-pvc
      - name: temp-storage
        emptyDir:
          sizeLimit: 10Gi
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-tesla-t4
```### è‡ª
å‹•æ“´å±•é…ç½®

```yaml
# kubernetes/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-customer-service-hpa
  namespace: ai-customer-service
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-customer-service-backend
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: custom.googleapis.com|ai_service|concurrent_requests
      target:
        type: AverageValue
        averageValue: "8"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60

---
# kubernetes/vpa.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: ai-customer-service-vpa
  namespace: ai-customer-service
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-customer-service-backend
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: flask-app
      minAllowed:
        cpu: 1000m
        memory: 2Gi
      maxAllowed:
        cpu: 8000m
        memory: 16Gi
      controlledResources: ["cpu", "memory"]
````

### GCP æˆæœ¬å„ªåŒ–ç­–ç•¥

````python
# æˆæœ¬å„ªåŒ–ç®¡ç†ç³»çµ±
class GCPCostOptimizer:
    def __init__(self, project_id, region="us-central1"):
        self.project_id = project_id
        self.region = region

    def setup_preemptible_instances(self):
        """è¨­ç½®å¯æ¶å å¯¦ä¾‹ä»¥é™ä½æˆæœ¬"""
        node_pool_config = {
            "name": "preemptible-pool",
            "initial_node_count": 1,
            "config": {
                "machine_type": "n1-standard-4",
                "disk_size_gb": 100,
                "disk_type": "pd-ssd",
                "preemptible": True,  # å¯æ¶å å¯¦ä¾‹ï¼Œæˆæœ¬é™ä½ 80%
                "oauth_scopes": [
                    "https://www.googleapis.com/auth/cloud-platform"
                ],
                "metadata": {
                    "disable-legacy-endpoints": "true"
                },
                "labels": {
                    "node-type": "preemptible",
                    "workload": "ai-inference"
                },
                "taints": [
                    {
                        "key": "preemptible",
                        "value": "true",
                        "effect": "NO_SCHEDULE"
                    }
                ]
            },
            "autoscaling": {
                "enabled": True,
                "min_node_count": 0,
                "max_node_count": 10
            },
            "management": {
                "auto_upgrade": True,
                "auto_repair": True
            }
        }
        return node_pool_config

    def setup_spot_instances(self):
        """è¨­ç½® Spot å¯¦ä¾‹é…ç½®"""
        spot_config = {
            "machine_type": "n1-standard-4",
            "provisioning_model": "SPOT",
            "instance_termination_action": "STOP",
            "max_run_duration": {
                "seconds": 3600  # 1å°æ™‚æœ€å¤§é‹è¡Œæ™‚é–“
            },
            "scheduling": {
                "preemptible": True,
                "automatic_restart": False,
                "on_host_maintenance": "TERMINATE"
            }
        }
        return spot_config

    def setup_scheduled_scaling(self):
        """è¨­ç½®å®šæ™‚æ“´å±•ç­–ç•¥"""
        # å·¥ä½œæ™‚é–“æ“´å±• (é€±ä¸€åˆ°é€±äº” 8:00-18:00)
        work_hours_schedule = {
            "name": "work-hours-scale-up",
            "schedule": "0 8 * * 1-5",  # æ¯é€±ä¸€åˆ°é€±äº”æ—©ä¸Š8é»
            "time_zone": "Asia/Taipei",
            "target": {
                "min_replicas": 5,
                "max_replicas": 20
            },
            "description": "Scale up during work hours"
        }

        # éå·¥ä½œæ™‚é–“ç¸®æ¸› (é€±ä¸€åˆ°é€±äº” 18:00-8:00)
        off_hours_schedule = {
            "name": "off-hours-scale-down",
            "schedule": "0 18 * * 1-5",  # æ¯é€±ä¸€åˆ°é€±äº”æ™šä¸Š6é»
            "time_zone": "Asia/Taipei",
            "target": {
                "min_replicas": 1,
                "max_replicas": 5
            },
            "description": "Scale down during off hours"
        }

        # é€±æœ«æœ€å°é…ç½®
        weekend_schedule = {
            "name": "weekend-minimal",
            "schedule": "0 0 * * 6,0",  # é€±å…­é€±æ—¥åˆå¤œ
            "time_zone": "Asia/Taipei",
            "target": {
                "min_replicas": 1,
                "max_replicas": 3
            },
            "description": "Minimal scaling for weekends"
        }

        return [work_hours_schedule, off_hours_schedule, weekend_schedule]

    def setup_budget_alerts(self):
        """è¨­ç½®é ç®—è­¦å ±"""
        budget_config = {
            "display_name": "AI Customer Service Budget",
            "budget_filter": {
                "projects": [f"projects/{self.project_id}"],
                "services": [
                    "services/6F81-5844-456A",  # Compute Engine
                    "services/95FF-2EF5-5EA1",  # Kubernetes Engine
                    "services/A1E8-BE35-7EBC"   # Cloud Storage
                ]
            },
            "amount": {
                "specified_amount": {
                    "currency_code": "USD",
                    "units": 1000  # $1000 USD per month
                }
            },
            "threshold_rules": [
                {
                    "threshold_percent": 0.5,  # 50% è­¦å ±
                    "spend_basis": "CURRENT_SPEND"
                },
                {
                    "threshold_percent": 0.8,  # 80% è­¦å ±
                    "spend_basis": "CURRENT_SPEND"
                },
                {
                    "threshold_percent": 1.0,  # 100% è­¦å ±
                    "spend_basis": "CURRENT_SPEND"
                }
            ]
        }
        return budget_config

    def optimize_storage_costs(self):
        """å„ªåŒ–å„²å­˜æˆæœ¬"""
        lifecycle_rules = [
            {
                "action": {"type": "SetStorageClass", "storageClass": "NEARLINE"},
                "condition": {
                    "age": 30,  # 30å¤©å¾Œè½‰ç‚º Nearline
                    "matchesPrefix": ["models/", "backups/"]
                }
            },
            {
                "action": {"type": "SetStorageClass", "storageClass": "COLDLINE"},
                "condition": {
                    "age": 90,  # 90å¤©å¾Œè½‰ç‚º Coldline
                    "matchesPrefix": ["backups/", "archives/"]
                }
            },
            {
                "action": {"type": "Delete"},
                "condition": {
                    "age": 365,  # 365å¤©å¾Œåˆªé™¤
                    "matchesPrefix": ["temp/", "cache/", "logs/"]
                }
            }
        ]
        return lifecycle_rules
```### GC
P éƒ¨ç½²æ­¥é©Ÿ

#### 1. ç’°å¢ƒæº–å‚™

```bash
# 1. å®‰è£ Google Cloud SDK
curl https://sdk.cloud.google.com | bash
exec -l $SHELL

# 2. åˆå§‹åŒ– gcloud
gcloud init
gcloud auth login
gcloud config set project YOUR_PROJECT_ID

# 3. å•Ÿç”¨å¿…è¦çš„ API
gcloud services enable container.googleapis.com
gcloud services enable storage.googleapis.com
gcloud services enable aiplatform.googleapis.com
gcloud services enable sql.googleapis.com
gcloud services enable monitoring.googleapis.com

# 4. å®‰è£ kubectl
gcloud components install kubectl

# 5. å‰µå»º GKE é›†ç¾¤
gcloud container clusters create ai-customer-service-cluster \
    --zone=us-central1-a \
    --num-nodes=3 \
    --enable-autoscaling \
    --min-nodes=1 \
    --max-nodes=10 \
    --machine-type=n1-standard-4 \
    --disk-size=100GB \
    --enable-autorepair \
    --enable-autoupgrade \
    --accelerator type=nvidia-tesla-t4,count=1
````

#### 2. å®¹å™¨åŒ–æ‡‰ç”¨

```dockerfile
# Dockerfile
FROM nvidia/cuda:11.8-devel-ubuntu20.04

# è¨­ç½®ç’°å¢ƒè®Šæ•¸
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_VISIBLE_DEVICES=0

# å®‰è£ç³»çµ±ä¾è³´
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3.10-pip \
    python3.10-dev \
    ffmpeg \
    libsndfile1 \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# è¨­ç½® Python åˆ¥å
RUN ln -s /usr/bin/python3.10 /usr/bin/python

# è¨­ç½®å·¥ä½œç›®éŒ„
WORKDIR /app

# è¤‡è£½éœ€æ±‚æª”æ¡ˆ
COPY requirements.txt .

# å®‰è£ Python ä¾è³´
RUN pip install --no-cache-dir -r requirements.txt

# å®‰è£ PyTorch with CUDA support
RUN pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# è¤‡è£½æ‡‰ç”¨ç¨‹å¼ç¢¼
COPY . .

# å‰µå»ºå¿…è¦ç›®éŒ„
RUN mkdir -p /app/models /app/audio_uploads /app/voice_output /app/temp /app/logs

# è¨­ç½®æ¬Šé™
RUN chmod +x /app/scripts/*.py

# å¥åº·æª¢æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:5000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 5000

# å•Ÿå‹•å‘½ä»¤
CMD ["python", "app.py"]
```

#### 3. å»ºæ§‹å’Œæ¨é€æ˜ åƒ

```bash
# 1. å»ºæ§‹ Docker æ˜ åƒ
docker build -t gcr.io/YOUR_PROJECT_ID/ai-customer-service:latest .

# 2. æ¨é€åˆ° Google Container Registry
docker push gcr.io/YOUR_PROJECT_ID/ai-customer-service:latest

# 3. æˆ–ä½¿ç”¨ Cloud Build
gcloud builds submit --tag gcr.io/YOUR_PROJECT_ID/ai-customer-service:latest .
```

#### 4. éƒ¨ç½²åˆ° GKE

```bash
# 1. ç²å–é›†ç¾¤æ†‘è­‰
gcloud container clusters get-credentials ai-customer-service-cluster --zone=us-central1-a

# 2. å‰µå»ºå‘½åç©ºé–“å’Œè³‡æº
kubectl apply -f kubernetes/namespace.yaml
kubectl apply -f kubernetes/configmap.yaml
kubectl apply -f kubernetes/secret.yaml
kubectl apply -f kubernetes/pvc.yaml

# 3. éƒ¨ç½²æ‡‰ç”¨
kubectl apply -f kubernetes/deployment.yaml
kubectl apply -f kubernetes/service.yaml
kubectl apply -f kubernetes/ingress.yaml

# 4. è¨­ç½®è‡ªå‹•æ“´å±•
kubectl apply -f kubernetes/hpa.yaml
kubectl apply -f kubernetes/vpa.yaml

# 5. é©—è­‰éƒ¨ç½²
kubectl get pods -n ai-customer-service
kubectl get services -n ai-customer-service
kubectl logs -f deployment/ai-customer-service-backend -n ai-customer-service
```

#### 5. è¨­ç½®ç›£æ§å’Œæ—¥èªŒ

```yaml
# kubernetes/monitoring.yaml
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: ai-customer-service-monitor
  namespace: ai-customer-service
spec:
  selector:
    matchLabels:
      app: ai-customer-service
  endpoints:
    - port: http
      path: /metrics
      interval: 30s

---
apiVersion: logging.coreos.com/v1
kind: ClusterLogForwarder
metadata:
  name: ai-service-logs
  namespace: ai-customer-service
spec:
  outputs:
    - name: gcp-logging
      type: googleCloudLogging
      googleCloudLogging:
        projectId: YOUR_PROJECT_ID
        logId: ai-customer-service
  pipelines:
    - name: application-logs
      inputRefs:
        - application
      outputRefs:
        - gcp-logging
```

---

## ğŸ”„ æ··åˆéƒ¨ç½²æ¶æ§‹

### æ¶æ§‹æ¦‚å¿µ

æ··åˆéƒ¨ç½²çµåˆäº†æœ¬åœ°ç«¯å’Œé›²ç«¯çš„å„ªå‹¢ï¼Œé©åˆéœ€è¦é€æ­¥é·ç§»åˆ°é›²ç«¯æˆ–æœ‰ç‰¹æ®Šåˆè¦è¦æ±‚çš„ä¼æ¥­ã€‚

### æ··åˆæ¶æ§‹è¨­è¨ˆ

````
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ··åˆæ¶æ§‹                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æœ¬åœ°ç«¯ (On-Premises)          â”‚  é›²ç«¯ (Cloud)          â”‚
â”‚  â”œâ”€â”€ æ•æ„Ÿæ•¸æ“šè™•ç†              â”‚  â”œâ”€â”€ æ¨¡å‹è¨“ç·´          â”‚
â”‚  â”œâ”€â”€ å³æ™‚æ¨ç†æœå‹™              â”‚  â”œâ”€â”€ å¤§è¦æ¨¡å„²å­˜        â”‚
â”‚  â”œâ”€â”€ æ ¸å¿ƒæ¥­å‹™é‚è¼¯              â”‚  â”œâ”€â”€ å‚™ä»½å’Œç½é›£æ¢å¾©    â”‚
â”‚  â””â”€â”€ ä½å»¶é²è¦æ±‚æœå‹™            â”‚  â””â”€â”€ å½ˆæ€§æ“´å±•æœå‹™      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              å®‰å…¨é€£æ¥ (VPN/å°ˆç·š)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```### æ··åˆéƒ¨
ç½²å¯¦æ–½æ–¹æ¡ˆ

```python
# æ··åˆéƒ¨ç½²ç®¡ç†ç³»çµ±
class HybridDeploymentManager:
    def __init__(self, local_config, cloud_config):
        self.local_manager = LocalModelManager(local_config['base_path'])
        self.cloud_manager = GCPModelManager(
            cloud_config['project_id'],
            cloud_config['region']
        )
        self.sync_config = {
            'sync_interval': 3600,  # 1å°æ™‚åŒæ­¥ä¸€æ¬¡
            'backup_to_cloud': True,
            'model_distribution': 'hybrid',  # local, cloud, hybrid
            'data_residency': 'local'  # æ•æ„Ÿæ•¸æ“šä¿ç•™åœ¨æœ¬åœ°
        }

    def distribute_models(self, model_type, staff_code=None):
        """æ™ºèƒ½åˆ†é…æ¨¡å‹åˆ°æœ¬åœ°æˆ–é›²ç«¯"""
        if model_type == 'emotion':
            # æƒ…ç·’è­˜åˆ¥æ¨¡å‹ä¿ç•™åœ¨æœ¬åœ°ä»¥é™ä½å»¶é²
            return 'local'
        elif model_type == 'voice':
            # èªéŸ³å…‹éš†æ¨¡å‹æ ¹æ“šä½¿ç”¨é »ç‡åˆ†é…
            usage_stats = self._get_model_usage_stats(staff_code)
            if usage_stats['daily_requests'] > 100:
                return 'local'  # é«˜é »ä½¿ç”¨ä¿ç•™åœ¨æœ¬åœ°
            else:
                return 'cloud'  # ä½é »ä½¿ç”¨æ”¾åœ¨é›²ç«¯
        elif model_type == 'pretrained':
            # é è¨“ç·´æ¨¡å‹åœ¨é›²ç«¯å„²å­˜ï¼Œæœ¬åœ°å¿«å–
            return 'cloud_with_cache'
        else:
            return 'local'

    def sync_models_to_cloud(self):
        """åŒæ­¥æœ¬åœ°æ¨¡å‹åˆ°é›²ç«¯å‚™ä»½"""
        local_models = self.local_manager.list_all_models()

        for model_info in local_models:
            model_type = model_info['type']
            staff_code = model_info.get('staff_code')
            local_path = model_info['path']

            # ç”Ÿæˆé›²ç«¯è·¯å¾‘
            if staff_code:
                cloud_path = f"backup/{model_type}/{staff_code}/{model_info['version']}"
            else:
                cloud_path = f"backup/{model_type}/{model_info['version']}"

            # æª¢æŸ¥æ˜¯å¦éœ€è¦åŒæ­¥
            if self._should_sync_model(model_info):
                print(f"åŒæ­¥æ¨¡å‹åˆ°é›²ç«¯: {local_path} -> {cloud_path}")
                self.cloud_manager.upload_model_to_gcs(
                    local_path, cloud_path, model_info['metadata']
                )

    def load_model_hybrid(self, model_type, staff_code=None, version='latest'):
        """æ··åˆæ¨¡å¼è¼‰å…¥æ¨¡å‹"""
        distribution = self.distribute_models(model_type, staff_code)

        if distribution == 'local':
            return self.local_manager.load_voice_model(staff_code, version)
        elif distribution == 'cloud':
            return self._load_from_cloud_with_cache(model_type, staff_code, version)
        elif distribution == 'cloud_with_cache':
            return self._load_with_intelligent_cache(model_type, staff_code, version)
        else:
            raise ValueError(f"æœªçŸ¥çš„åˆ†é…ç­–ç•¥: {distribution}")

    def _load_from_cloud_with_cache(self, model_type, staff_code, version):
        """å¾é›²ç«¯è¼‰å…¥æ¨¡å‹ä¸¦å¿«å–åˆ°æœ¬åœ°"""
        cache_key = f"{model_type}_{staff_code}_{version}"
        local_cache_path = f"./cache/models/{cache_key}"

        # æª¢æŸ¥æœ¬åœ°å¿«å–
        if os.path.exists(local_cache_path):
            cache_age = time.time() - os.path.getmtime(local_cache_path)
            if cache_age < 3600:  # å¿«å–æœ‰æ•ˆæœŸ 1 å°æ™‚
                return self._load_from_cache(local_cache_path)

        # å¾é›²ç«¯ä¸‹è¼‰åˆ°å¿«å–
        if staff_code:
            cloud_path = f"voice_models/{staff_code}/v{version}"
        else:
            cloud_path = f"{model_type}_models/v{version}"

        self.cloud_manager.download_model_from_gcs(cloud_path, local_cache_path)
        return self._load_from_cache(local_cache_path)

    def setup_data_residency_rules(self):
        """è¨­ç½®æ•¸æ“šé§ç•™è¦å‰‡"""
        rules = {
            'sensitive_data': {
                'location': 'local_only',
                'encryption': 'required',
                'backup_allowed': False
            },
            'model_data': {
                'location': 'hybrid',
                'encryption': 'optional',
                'backup_allowed': True
            },
            'training_data': {
                'location': 'local_primary',
                'encryption': 'required',
                'backup_allowed': True,
                'backup_location': 'cloud_encrypted'
            },
            'inference_logs': {
                'location': 'local',
                'retention_days': 30,
                'archive_to_cloud': True
            }
        }
        return rules

    def monitor_hybrid_performance(self):
        """ç›£æ§æ··åˆéƒ¨ç½²æ€§èƒ½"""
        metrics = {
            'local_latency': self._measure_local_latency(),
            'cloud_latency': self._measure_cloud_latency(),
            'cache_hit_rate': self._calculate_cache_hit_rate(),
            'sync_status': self._check_sync_status(),
            'cost_breakdown': self._calculate_cost_breakdown()
        }
        return metrics
````

### æ··åˆéƒ¨ç½²é…ç½®æª”æ¡ˆ

```yaml
# config/hybrid_config.yaml
hybrid_deployment:
  local:
    enabled: true
    base_path: "./models"
    cache_size: "2GB"
    gpu_enabled: true
    services:
      - emotion_recognition
      - real_time_inference
      - sensitive_data_processing

  cloud:
    enabled: true
    provider: "gcp"
    project_id: "your-project-id"
    region: "us-central1"
    services:
      - model_training
      - large_scale_storage
      - backup_and_recovery
      - batch_processing

  sync:
    interval_seconds: 3600
    backup_enabled: true
    compression_enabled: true
    encryption_enabled: true

  data_residency:
    sensitive_data: "local_only"
    model_weights: "hybrid"
    training_data: "local_primary_cloud_backup"
    inference_logs: "local_with_cloud_archive"

  failover:
    enabled: true
    primary: "local"
    secondary: "cloud"
    health_check_interval: 30
    failover_threshold: 3
```

---

## ğŸ“Š éƒ¨ç½²æ–¹æ¡ˆæ¯”è¼ƒèˆ‡é¸æ“‡

### è©³ç´°æ¯”è¼ƒè¡¨

| ç‰¹æ€§             | æœ¬åœ°ç«¯éƒ¨ç½²      | é›²ç«¯éƒ¨ç½² (GCP) | æ··åˆéƒ¨ç½²      |
| ---------------- | --------------- | -------------- | ------------- |
| **åˆæœŸæŠ•è³‡**     | ä¸­ç­‰ ($20K-50K) | ä½ ($5K-10K)   | é«˜ ($30K-80K) |
| **æœˆåº¦é‹ç‡Ÿæˆæœ¬** | ä½ ($500-2K)    | ä¸­ç­‰ ($2K-10K) | ä¸­ç­‰ ($1K-6K) |
| **æ“´å±•æ€§**       | æœ‰é™ (æ‰‹å‹•)     | ç„¡é™ (è‡ªå‹•)    | é«˜ (æ™ºèƒ½)     |
| **å»¶é²**         | æ¥µä½ (<100ms)   | ä½ (100-300ms) | æ¥µä½ (<100ms) |
| **å¯ç”¨æ€§**       | ä¸­ç­‰ (95-98%)   | é«˜ (99.9%+)    | é«˜ (99.5%+)   |
| **å®‰å…¨æ€§**       | é«˜ (è‡ªæ§)       | æ¥µé«˜ (ä¼æ¥­ç´š)  | æ¥µé«˜ (é›™é‡)   |
| **ç¶­è­·è¤‡é›œåº¦**   | é«˜              | ä½             | ä¸­ç­‰          |
| **æ•¸æ“šæ§åˆ¶**     | å®Œå…¨            | æœ‰é™           | é«˜            |
| **ç½é›£æ¢å¾©**     | æ‰‹å‹•            | è‡ªå‹•           | è‡ªå‹•          |
| **åˆè¦æ€§**       | é«˜              | ä¸­ç­‰           | é«˜            |

### é¸æ“‡å»ºè­°

#### é¸æ“‡æœ¬åœ°ç«¯éƒ¨ç½²çš„æƒ…æ³ï¼š

- **å°å‹åœ˜éšŠ** (5-20 äºº)
- **é ç®—æœ‰é™** çš„åˆå‰µä¼æ¥­
- **æ•¸æ“šæ•æ„Ÿæ€§æ¥µé«˜** çš„è¡Œæ¥­ (é‡‘èã€é†«ç™‚)
- **ç¶²è·¯ç’°å¢ƒä¸ç©©å®š** çš„åœ°å€
- **å°å»¶é²è¦æ±‚æ¥µé«˜** çš„æ‡‰ç”¨

#### é¸æ“‡é›²ç«¯éƒ¨ç½²çš„æƒ…æ³ï¼š

- **å¤§å‹ä¼æ¥­** (100 äººä»¥ä¸Š)
- **å¿«é€Ÿæ“´å±•** çš„æ¥­å‹™éœ€æ±‚
- **å…¨çƒåŒ–æœå‹™** éœ€æ±‚
- **æœ‰é™çš„ IT ç¶­è­·èƒ½åŠ›**
- **éœ€è¦é«˜å¯ç”¨æ€§** çš„é—œéµæ¥­å‹™

#### é¸æ“‡æ··åˆéƒ¨ç½²çš„æƒ…æ³ï¼š

- **ä¸­å¤§å‹ä¼æ¥­** (50-500 äºº)
- **é€æ­¥é›²ç«¯é·ç§»** ç­–ç•¥
- **è¤‡é›œçš„åˆè¦è¦æ±‚**
- **ä¸åŒæ•æ„Ÿåº¦çš„æ•¸æ“š** éœ€è¦åˆ†åˆ¥è™•ç†
- **éœ€è¦æœ€ä½³æ€§åƒ¹æ¯”** çš„æ–¹æ¡ˆ#

## é·ç§»è·¯å¾‘å»ºè­°

#### éšæ®µä¸€ï¼šæœ¬åœ°ç«¯å„ªåŒ– (0-6 å€‹æœˆ)

```
ç›®æ¨™ï¼šå»ºç«‹ç©©å®šçš„æœ¬åœ°ç«¯ç³»çµ±
â”œâ”€â”€ å®Œå–„æœ¬åœ°æ¨¡å‹ç®¡ç†ç³»çµ±
â”œâ”€â”€ å»ºç«‹æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶æ©Ÿåˆ¶
â”œâ”€â”€ å¯¦ä½œåŸºç¤ç›£æ§å’Œå‚™ä»½
â”œâ”€â”€ å„ªåŒ–æ¨ç†æ€§èƒ½
â””â”€â”€ å»ºç«‹é‹ç¶­æµç¨‹
```

#### éšæ®µäºŒï¼šæ··åˆéƒ¨ç½² (6-12 å€‹æœˆ)

```
ç›®æ¨™ï¼šé€æ­¥å¼•å…¥é›²ç«¯èƒ½åŠ›
â”œâ”€â”€ æ¨¡å‹å„²å­˜é·ç§»åˆ° Cloud Storage
â”œâ”€â”€ è³‡æ–™åº«é·ç§»åˆ° Cloud SQL
â”œâ”€â”€ å»ºç«‹é›²ç«¯å‚™ä»½æ©Ÿåˆ¶
â”œâ”€â”€ å¯¦ä½œæ™ºèƒ½å¿«å–ç­–ç•¥
â””â”€â”€ è¨­ç½®ç›£æ§å’Œå‘Šè­¦
```

#### éšæ®µä¸‰ï¼šå…¨é›²ç«¯éƒ¨ç½² (12-18 å€‹æœˆ)

```
ç›®æ¨™ï¼šå®Œå…¨é›²ç«¯åŒ–é‹ç‡Ÿ
â”œâ”€â”€ æ¨ç†æœå‹™é·ç§»åˆ° GKE
â”œâ”€â”€ ä½¿ç”¨ Vertex AI é€²è¡Œæ¨¡å‹ç®¡ç†
â”œâ”€â”€ å¯¦ä½œè‡ªå‹•æ“´å±•å’Œæˆæœ¬å„ªåŒ–
â”œâ”€â”€ å»ºç«‹ CI/CD æµæ°´ç·š
â””â”€â”€ å®Œå–„ç½é›£æ¢å¾©æ©Ÿåˆ¶
```

---

## ğŸ”§ é‹ç¶­å’Œç›£æ§

### ç³»çµ±ç›£æ§æŒ‡æ¨™

#### æ ¸å¿ƒæ€§èƒ½æŒ‡æ¨™

```python
# ç›£æ§æŒ‡æ¨™å®šç¾©
MONITORING_METRICS = {
    'system_performance': {
        'cpu_usage': {'threshold': 80, 'unit': '%'},
        'memory_usage': {'threshold': 85, 'unit': '%'},
        'gpu_usage': {'threshold': 90, 'unit': '%'},
        'disk_usage': {'threshold': 85, 'unit': '%'},
        'network_io': {'threshold': 1000, 'unit': 'Mbps'}
    },
    'application_performance': {
        'request_latency': {'threshold': 2000, 'unit': 'ms'},
        'throughput': {'threshold': 100, 'unit': 'req/min'},
        'error_rate': {'threshold': 1, 'unit': '%'},
        'model_load_time': {'threshold': 30, 'unit': 's'},
        'inference_time': {'threshold': 1500, 'unit': 'ms'}
    },
    'business_metrics': {
        'active_models': {'threshold': None, 'unit': 'count'},
        'daily_requests': {'threshold': None, 'unit': 'count'},
        'user_satisfaction': {'threshold': 4.0, 'unit': 'score'},
        'model_accuracy': {'threshold': 85, 'unit': '%'},
        'cache_hit_rate': {'threshold': 80, 'unit': '%'}
    }
}
```

#### ç›£æ§ç³»çµ±å¯¦ä½œ

```python
# ç›£æ§ç³»çµ±
class SystemMonitor:
    def __init__(self, config):
        self.config = config
        self.metrics_collector = MetricsCollector()
        self.alert_manager = AlertManager()

    def collect_system_metrics(self):
        """æ”¶é›†ç³»çµ±æŒ‡æ¨™"""
        import psutil
        import GPUtil

        metrics = {
            'timestamp': datetime.now().isoformat(),
            'cpu_usage': psutil.cpu_percent(interval=1),
            'memory_usage': psutil.virtual_memory().percent,
            'disk_usage': psutil.disk_usage('/').percent,
            'network_io': self._get_network_io(),
            'gpu_metrics': self._get_gpu_metrics()
        }

        return metrics

    def collect_application_metrics(self):
        """æ”¶é›†æ‡‰ç”¨æŒ‡æ¨™"""
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'active_connections': self._get_active_connections(),
            'request_queue_size': self._get_request_queue_size(),
            'model_cache_usage': self._get_cache_usage(),
            'error_count': self._get_error_count(),
            'response_times': self._get_response_times()
        }

        return metrics

    def check_health(self):
        """å¥åº·æª¢æŸ¥"""
        health_status = {
            'status': 'healthy',
            'checks': {},
            'timestamp': datetime.now().isoformat()
        }

        # æª¢æŸ¥è³‡æ–™åº«é€£æ¥
        try:
            # åŸ·è¡Œç°¡å–®æŸ¥è©¢
            health_status['checks']['database'] = 'healthy'
        except Exception as e:
            health_status['checks']['database'] = f'unhealthy: {str(e)}'
            health_status['status'] = 'unhealthy'

        # æª¢æŸ¥æ¨¡å‹è¼‰å…¥ç‹€æ…‹
        try:
            # æª¢æŸ¥é—œéµæ¨¡å‹æ˜¯å¦è¼‰å…¥
            health_status['checks']['models'] = 'healthy'
        except Exception as e:
            health_status['checks']['models'] = f'unhealthy: {str(e)}'
            health_status['status'] = 'unhealthy'

        # æª¢æŸ¥ç£ç¢Ÿç©ºé–“
        disk_usage = psutil.disk_usage('/').percent
        if disk_usage > 90:
            health_status['checks']['disk_space'] = f'warning: {disk_usage}% used'
            health_status['status'] = 'warning'
        else:
            health_status['checks']['disk_space'] = 'healthy'

        return health_status

    def setup_alerts(self):
        """è¨­ç½®å‘Šè­¦è¦å‰‡"""
        alert_rules = [
            {
                'name': 'high_cpu_usage',
                'condition': 'cpu_usage > 80',
                'severity': 'warning',
                'notification': ['email', 'slack']
            },
            {
                'name': 'high_memory_usage',
                'condition': 'memory_usage > 85',
                'severity': 'warning',
                'notification': ['email', 'slack']
            },
            {
                'name': 'high_error_rate',
                'condition': 'error_rate > 5',
                'severity': 'critical',
                'notification': ['email', 'slack', 'sms']
            },
            {
                'name': 'model_inference_slow',
                'condition': 'inference_time > 3000',
                'severity': 'warning',
                'notification': ['email']
            }
        ]

        for rule in alert_rules:
            self.alert_manager.add_rule(rule)
```

### æ—¥èªŒç®¡ç†

```python
# æ—¥èªŒç®¡ç†ç³»çµ±
import logging
import json
from datetime import datetime

class StructuredLogger:
    def __init__(self, name, level=logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)

        # å‰µå»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )

        # æ–‡ä»¶è™•ç†å™¨
        file_handler = logging.FileHandler('logs/ai_service.log')
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)

        # æ§åˆ¶å°è™•ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)

    def log_request(self, request_id, endpoint, method, user_id=None):
        """è¨˜éŒ„è«‹æ±‚æ—¥èªŒ"""
        log_data = {
            'event_type': 'request',
            'request_id': request_id,
            'endpoint': endpoint,
            'method': method,
            'user_id': user_id,
            'timestamp': datetime.now().isoformat()
        }
        self.logger.info(json.dumps(log_data, ensure_ascii=False))

    def log_model_inference(self, model_type, staff_code, inference_time, success=True):
        """è¨˜éŒ„æ¨¡å‹æ¨ç†æ—¥èªŒ"""
        log_data = {
            'event_type': 'model_inference',
            'model_type': model_type,
            'staff_code': staff_code,
            'inference_time_ms': inference_time,
            'success': success,
            'timestamp': datetime.now().isoformat()
        }
        self.logger.info(json.dumps(log_data, ensure_ascii=False))

    def log_error(self, error_type, error_message, stack_trace=None, context=None):
        """è¨˜éŒ„éŒ¯èª¤æ—¥èªŒ"""
        log_data = {
            'event_type': 'error',
            'error_type': error_type,
            'error_message': error_message,
            'stack_trace': stack_trace,
            'context': context,
            'timestamp': datetime.now().isoformat()
        }
        self.logger.error(json.dumps(log_data, ensure_ascii=False))
```

---

## ğŸ” å®‰å…¨æ€§æœ€ä½³å¯¦è¸

### æ¨¡å‹å®‰å…¨

```python
# æ¨¡å‹å®‰å…¨ç®¡ç†
class ModelSecurityManager:
    def __init__(self):
        self.encryption_key = self._generate_encryption_key()
        self.access_control = AccessControlManager()

    def encrypt_model(self, model_path, encrypted_path):
        """åŠ å¯†æ¨¡å‹æª”æ¡ˆ"""
        from cryptography.fernet import Fernet

        fernet = Fernet(self.encryption_key)

        with open(model_path, 'rb') as file:
            original_data = file.read()

        encrypted_data = fernet.encrypt(original_data)

        with open(encrypted_path, 'wb') as encrypted_file:
            encrypted_file.write(encrypted_data)

        # åˆªé™¤åŸå§‹æª”æ¡ˆ
        os.remove(model_path)

        return encrypted_path

    def decrypt_model(self, encrypted_path, decrypted_path):
        """è§£å¯†æ¨¡å‹æª”æ¡ˆ"""
        from cryptography.fernet import Fernet

        fernet = Fernet(self.encryption_key)

        with open(encrypted_path, 'rb') as encrypted_file:
            encrypted_data = encrypted_file.read()

        decrypted_data = fernet.decrypt(encrypted_data)

        with open(decrypted_path, 'wb') as decrypted_file:
            decrypted_file.write(decrypted_data)

        return decrypted_path

    def validate_model_integrity(self, model_path, expected_hash):
        """é©—è­‰æ¨¡å‹å®Œæ•´æ€§"""
        import hashlib

        sha256_hash = hashlib.sha256()
        with open(model_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)

        actual_hash = sha256_hash.hexdigest()

        if actual_hash != expected_hash:
            raise SecurityError(f"æ¨¡å‹å®Œæ•´æ€§é©—è­‰å¤±æ•—: {actual_hash} != {expected_hash}")

        return True

    def audit_model_access(self, user_id, model_id, action, success=True):
        """ç¨½æ ¸æ¨¡å‹å­˜å–"""
        audit_log = {
            'timestamp': datetime.now().isoformat(),
            'user_id': user_id,
            'model_id': model_id,
            'action': action,
            'success': success,
            'ip_address': self._get_client_ip(),
            'user_agent': self._get_user_agent()
        }

        # å¯«å…¥ç¨½æ ¸æ—¥èªŒ
        with open('logs/model_access_audit.log', 'a') as f:
            f.write(json.dumps(audit_log) + '\n')
```

### ç¶²è·¯å®‰å…¨

```yaml
# ç¶²è·¯å®‰å…¨é…ç½®
network_security:
  firewall_rules:
    - name: "allow-http"
      direction: "INGRESS"
      ports: ["80"]
      source_ranges: ["0.0.0.0/0"]
    - name: "allow-https"
      direction: "INGRESS"
      ports: ["443"]
      source_ranges: ["0.0.0.0/0"]
    - name: "allow-internal"
      direction: "INGRESS"
      ports: ["5000"]
      source_ranges: ["10.0.0.0/8"]

  ssl_configuration:
    enabled: true
    certificate_path: "/etc/ssl/certs/ai-service.crt"
    private_key_path: "/etc/ssl/private/ai-service.key"
    protocols: ["TLSv1.2", "TLSv1.3"]
    ciphers: ["ECDHE-RSA-AES256-GCM-SHA384", "ECDHE-RSA-AES128-GCM-SHA256"]

  rate_limiting:
    enabled: true
    requests_per_minute: 100
    burst_size: 20
    whitelist_ips: ["127.0.0.1", "10.0.0.0/8"]
```

---

## ğŸ“ˆ æ€§èƒ½å„ªåŒ–

### æ¨¡å‹è¼‰å…¥å„ªåŒ–

```python
# æ¨¡å‹è¼‰å…¥å„ªåŒ–
class OptimizedModelLoader:
    def __init__(self):
        self.model_cache = {}
        self.preload_queue = Queue()
        self.cache_stats = {'hits': 0, 'misses': 0}

    def preload_models(self, model_list):
        """é è¼‰å…¥å¸¸ç”¨æ¨¡å‹"""
        for model_info in model_list:
            threading.Thread(
                target=self._preload_single_model,
                args=(model_info,)
            ).start()

    def _preload_single_model(self, model_info):
        """é è¼‰å…¥å–®å€‹æ¨¡å‹"""
        try:
            model_key = f"{model_info['type']}_{model_info.get('staff_code', 'default')}"

            if model_info['type'] == 'emotion':
                model = self._load_emotion_model(model_info)
            elif model_info['type'] == 'voice':
                model = self._load_voice_model(model_info)

            self.model_cache[model_key] = {
                'model': model,
                'loaded_at': time.time(),
                'access_count': 0
            }

            print(f"é è¼‰å…¥æ¨¡å‹å®Œæˆ: {model_key}")

        except Exception as e:
            print(f"é è¼‰å…¥æ¨¡å‹å¤±æ•— {model_info}: {e}")

    def get_model(self, model_type, staff_code=None):
        """ç²å–æ¨¡å‹ (å¸¶å¿«å–)"""
        model_key = f"{model_type}_{staff_code or 'default'}"

        if model_key in self.model_cache:
            self.cache_stats['hits'] += 1
            self.model_cache[model_key]['access_count'] += 1
            return self.model_cache[model_key]['model']

        self.cache_stats['misses'] += 1

        # è¼‰å…¥æ¨¡å‹
        if model_type == 'emotion':
            model = self._load_emotion_model({'type': model_type})
        elif model_type == 'voice':
            model = self._load_voice_model({'type': model_type, 'staff_code': staff_code})

        # åŠ å…¥å¿«å–
        self.model_cache[model_key] = {
            'model': model,
            'loaded_at': time.time(),
            'access_count': 1
        }

        return model

    def cleanup_cache(self, max_age_hours=2):
        """æ¸…ç†éæœŸå¿«å–"""
        current_time = time.time()
        expired_keys = []

        for key, cache_info in self.model_cache.items():
            age_hours = (current_time - cache_info['loaded_at']) / 3600
            if age_hours > max_age_hours and cache_info['access_count'] < 5:
                expired_keys.append(key)

        for key in expired_keys:
            del self.model_cache[key]
            print(f"æ¸…ç†éæœŸå¿«å–: {key}")

    def get_cache_stats(self):
        """ç²å–å¿«å–çµ±è¨ˆ"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = self.cache_stats['hits'] / total_requests if total_requests > 0 else 0

        return {
            'hit_rate': hit_rate,
            'total_requests': total_requests,
            'cached_models': len(self.model_cache),
            'cache_size_mb': self._calculate_cache_size()
        }
```

é€™ä»½å®Œæ•´çš„æ¨¡å‹å„²å­˜èˆ‡éƒ¨ç½²æŒ‡å—æ¶µè“‹äº†ï¼š

1. **æœ¬åœ°ç«¯éƒ¨ç½²æ–¹æ¡ˆ** - å®Œæ•´çš„ç¡¬é«”éœ€æ±‚ã€å®‰è£æ­¥é©Ÿå’Œé…ç½®
2. **GCP é›²ç«¯éƒ¨ç½²æ–¹æ¡ˆ** - Kubernetesã€è‡ªå‹•æ“´å±•ã€æˆæœ¬å„ªåŒ–
3. **æ··åˆéƒ¨ç½²æ¶æ§‹** - çµåˆæœ¬åœ°å’Œé›²ç«¯å„ªå‹¢çš„è§£æ±ºæ–¹æ¡ˆ
4. **é‹ç¶­ç›£æ§** - ç³»çµ±ç›£æ§ã€æ—¥èªŒç®¡ç†ã€å¥åº·æª¢æŸ¥
5. **å®‰å…¨æ€§æœ€ä½³å¯¦è¸** - æ¨¡å‹åŠ å¯†ã€å­˜å–æ§åˆ¶ã€ç¨½æ ¸æ©Ÿåˆ¶
6. **æ€§èƒ½å„ªåŒ–** - æ¨¡å‹å¿«å–ã€é è¼‰å…¥ã€è³‡æºç®¡ç†

é€™å€‹æŒ‡å—ç‚ºä¸åŒè¦æ¨¡å’Œéœ€æ±‚çš„ä¼æ¥­æä¾›äº†å®Œæ•´çš„éƒ¨ç½²é¸æ“‡å’Œå¯¦æ–½è·¯å¾‘ã€‚
